{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Tensorflow\n",
    "<p class='lead'>\n",
    "Author: Oliveira, Markos F. B. G.<br />\n",
    "</p>\n",
    "\n",
    "# Description\n",
    "\n",
    "This document is based on Chapter 9 content of Geron's book *Hands-On Machine Learning with Scikit-Learn & TensorFlow*. It covers only the basics of the related topics and it's not meant to be a tutorial. Please, refer to the referenced book and scikit-learn/tensorflow online documentation for specific information.\n",
    "\n",
    "In paticular, this notebook provides an example of applying linear regression using Tensorflow on moderated-size regression problem. It applies stochastic gradient descent using mini-batches of a given size, two approaches for selecting the next mini-batch exist: *1*: random indices from the training set are selected to be inside the mini-batch using a randomized seed, which is the number of the training iteration. In this case, one epoch, in general, does not cover all the training instances. *2*: a random permutation of the examples is passed and a particular slice of this permutation is used as mini-batch. It's guarateed that all examples are used in any epoch (this approach is the most common).\n",
    "\n",
    "The algorithm that minimizes the cost function is a plain stochastic gradient descent with mean squared error. An automatic search for good hyperparameters is not implemented, nor ensemble methods are used. However, the algorithm employs a naive implementation of early stopping. It just checks if the validaiton error has not decreased by an amount of *n_it* iterations; if dows not, the training is stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'feature_names', 'target', 'DESCR'])\n",
      "Entire dataset shape:  (20640, 8)\n"
     ]
    }
   ],
   "source": [
    "#Fetching the data: in the first time it's executed, the data is downloaded to ‘~/scikit_learn_data’ subfolders (this may\n",
    "#take a while); once, downloaded and executed again, the function just read the available dataset.\n",
    "housing = fetch_california_housing()\n",
    "print(fetch_california_housing().keys())\n",
    "print('Entire dataset shape: ', housing.data.shape)\n",
    "\n",
    "#Splitting the dataset into training, validation (to early stopping and testing):\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(housing.data, housing.target, random_state=0, test_size=.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0, test_size=.2) #25% of the 80%\n",
    "#is 20% of the original, so: X_train_ratio = 0.6, X_val_ratio = 0.2, X_test_ratio = 0.2\n",
    "\n",
    "#Normalizing the data:\n",
    "scaler = StandardScaler() #instantiates the scaler.\n",
    "scaler.fit(X_train) #fits the training data, i.e. computes the necessary parameters.\n",
    "X_train_scaled = scaler.transform(X_train) #Transform the datasets based on parameters evaluated on training set only.\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#It's important to transform the y_train and y_test 1d-array ((m,) format) to 2d-array ((m,1) format):\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "#Adding the bias feature to each example (necessary for linear regression):\n",
    "X_train_scaled_plus_bias = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_val_scaled_plus_bias = np.c_[np.ones((X_val_scaled.shape[0], 1)), X_val_scaled]\n",
    "X_test_scaled_plus_bias = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "m, n = X_train_scaled_plus_bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size, idxs, approach=2):\n",
    "    \"\"\"Returns the mini-batch (X, y) for a training step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : int\n",
    "        Number of epoch.\n",
    "    batch_index : int\n",
    "        Number of batch inside the epoch.\n",
    "    batch_size : int\n",
    "        Batch size.\n",
    "    idxs : list of int\n",
    "        Permutation of training set indices.\n",
    "    approach: int, optional\n",
    "        Approach used for chosing the mini-batches. \n",
    "    Returns\n",
    "    -------\n",
    "    X_batch : np.ndarray\n",
    "        Mini-batch of inputs.\n",
    "    y_batch : np.ndarray\n",
    "        Mini-batch of outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    if approach == 1:\n",
    "        rnd.seed(epoch * n_batches + batch_index)\n",
    "        indices = rnd.randint(m, size=batch_size)\n",
    "    elif approach == 2:\n",
    "        indices = idxs[batch_index * batch_size: (batch_index + 1) * batch_size]\n",
    "    \n",
    "    X_batch = X_train_scaled_plus_bias[indices]\n",
    "    y_batch = y_train[indices]\n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression using TensorFlow\n",
    "\n",
    "Below, the user can set some running parameters acording to their preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SGD parameters:\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.005\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "#Early-stopping:\n",
    "early_stopping = True #enable early stopping.\n",
    "min_val_error = float(\"inf\") #stores the best error so far.\n",
    "min_val_error_epoch = 0 #epoch when minimum validation error was found.\n",
    "n_it = 2*batch_size #maximum number of iterations without improvement in validation error.\n",
    "imp_counter = 0 #counts the umber of iterations there is no improvement in the validation error.\n",
    "stop_learning = True #enable stop learning after 'n_it' iterations of no improvement in validation error. If 'False', all\n",
    "#epochs are runned, but the model which least validation error is returned.\n",
    "\n",
    "#Monitoring:\n",
    "prt = True #enable print training statistics (in this case MSE).\n",
    "print_step = 100 # number of epochs to periodically print training statistics.\n",
    "\n",
    "#Saving/restoring TF model:\n",
    "restore = False #enable model's restoration.\n",
    "path_restore = '/tmp/finals/my_model_final.ckpt' #path of original model to be restored.\n",
    "save_ckpt = False #enable saving training checkpoints.\n",
    "saver_step = 100 # number of epochs to periodically save model's checkpoint.\n",
    "\n",
    "#Tensorboard logs:\n",
    "tb = False #enable log training statistics for tensorboard.\n",
    "tensorboard_step = 100 #number of epochs to periodically log statistics in tensorboard log files.\n",
    "root_logdir = \"tf_logs\" #external folder where all logging stats will be placed (for different session runs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing and Executing the Tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training MSE = 1.369, Validation MSE = 1.531\n",
      "Epoch 100, Training MSE = 0.5234, Validation MSE = 0.5306\n",
      "Epoch 200, Training MSE = 0.5227, Validation MSE = 0.5341\n",
      "Epoch 300, Training MSE = 0.5238, Validation MSE = 0.5449\n",
      "Epoch 139 with minimum validation error 0.5239.\n",
      "Best model saved as: ./tmp/best_model-20170815205237.ckpt\n",
      "Training time: 21.45600 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #restoring the default graph.\n",
    "tf.logging.set_verbosity(tf.logging.WARN) #supress TF logging messages when saving ckpt files.\n",
    "start_time = time.time()\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now) #relative path of tensorboard logs for a particular run (current time\n",
    "#is used so that each folder has different running stats, comparision between them can be made inside tensorboard).\n",
    "\n",
    "# TF CONSTRUCTION PHASE\n",
    "\n",
    "# 1- Creating variables, placeholders and constants.\n",
    "X = tf.placeholder(tf.float32, shape=(None, n), name=\"X\") #n contains the bias already.\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0, seed=1), name=\"theta\")\n",
    "\n",
    "# 2- Creating operations.\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "with tf.name_scope(\"loss\") as scope: # Grouping related nodes to the same scope in the TF-graph.\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# 3- Node that initialize the variables.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 4- Creating the saver.\n",
    "saver_save = tf.train.Saver(max_to_keep=None) #save all variable values.\n",
    "if restore:\n",
    "    saver_restore = tf.train.Saver() #restore all variable values.\n",
    "    #saver_restore = tf.train.Saver({\"weights\": theta}) #restore only the old theta variable under the name of 'weights'.\n",
    "\n",
    "# 5- Tensorboard definitions:\n",
    "if tb:\n",
    "    mse_summary = tf.summary.scalar('MSE', mse) #creates a node in the graph that will evaluate the MSE value\n",
    "    #and write it to a TensorBoard-compatible binary log string called a summary.\n",
    "    summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) #creates a FileWriter that you will\n",
    "    #use to write summaries to logfiles in the log directory.\n",
    "\n",
    "\n",
    "# TF EXECUTION PHASE:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if restore:  #to restore a model the construction phase must be identical than the one used to save it. \n",
    "        saver_restore.restore(sess, path_restore)\n",
    "    else:\n",
    "        sess.run(init) #initializing the variables.\n",
    "    \n",
    "    breaker = False #used to break the epoch loop if the number of iterations without validation improvement increases above\n",
    "    #the threshold 'n_int'.\n",
    "    for epoch in range(n_epochs+1): #for each epoch..\n",
    "        \n",
    "        idXs = np.random.permutation(range(m)) #creates a random permutation of the instances.\n",
    "        \n",
    "        for batch_index in range(n_batches-1): # for each mini-batch.. The '-1' guarantees all batches are equally sized, including\n",
    "            #the last one, without this the last batch with few examples could be used to update the weights. Hence, a few points\n",
    "            #(< batch_size) may be out of each epoch.\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size, idXs)\n",
    "            #Training step:\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "        #Early-stopping (we could implement early stopping after each update (iteration), however, it would be too slow):\n",
    "        if (early_stopping):\n",
    "            val_error = mse.eval(feed_dict={X: X_val_scaled_plus_bias, y: y_val})\n",
    "            imp_counter = imp_counter + 1 \n",
    "            if val_error <= min_val_error:\n",
    "                min_val_error = val_error\n",
    "                min_val_error_epoch = epoch\n",
    "                save_path = saver_save.save(sess, \"./tmp/best_model-{}.ckpt\".format(now))\n",
    "                imp_counter = 0\n",
    "            elif (stop_learning) and (imp_counter >= n_it):\n",
    "                breaker = True\n",
    "                break    \n",
    "                    \n",
    "        if breaker:\n",
    "            break \n",
    "        if (prt) and (epoch % print_step == 0):\n",
    "            print(\"Epoch {}, Training MSE = {:.4}, Validation MSE = {:.4}\".format(epoch,\n",
    "                                                                     mse.eval(feed_dict={X: X_train_scaled_plus_bias, y: y_train}),\n",
    "                                                                     mse.eval(feed_dict={X: X_val_scaled_plus_bias, y: y_val})))\n",
    "        if (tb) and (epoch % tensorboard_step == 0):\n",
    "            summary_str_val = mse_summary.eval(feed_dict={X: X_val_scaled_plus_bias, y: y_cal})   \n",
    "            summary_str_training = mse_summary.eval(feed_dict={X: X_train_scaled_plus_bias, y: y_train}) # This will output a\n",
    "            #summary that you can then write to the events file using the file_writer.\n",
    "            step = epoch * n_batches + batch_index\n",
    "            summary_writer.add_summary(summary_str_training, step)\n",
    "            summary_writer.add_summary(summary_str_val, step)\n",
    "        if (save_ckpt) and (epoch % saver_step == 0):\n",
    "            save_path = saver_save.save(sess, \"./tmp/my_model-{}.ckpt\".format(epoch))\n",
    "          \n",
    "    #Getting the 'best theta': this will probably not be the best theta over the training set (forget about the test\n",
    "    #set), because we are updating the weights according the mini batches instead of the full training set error.\n",
    "    #best_theta = theta.eval()\n",
    "    #final_error = mse.eval(feed_dict={X: X_test_scaled_plus_bias, y: y_test})\n",
    "    if not (early_stopping):\n",
    "        save_path = saver_save.save(sess, \"./tmp/best_model-{}.ckpt\".format(now))\n",
    "\n",
    "#Flushing and closing FileWriter if necessary:          \n",
    "if tb:                            \n",
    "    summary_writer.flush()\n",
    "    summary_writer.close() \n",
    "\n",
    "if early_stopping:\n",
    "    print('Epoch {} with minimum validation error {:.4}.'.format(min_val_error_epoch, min_val_error))\n",
    "best_model_path = \"./tmp/best_model-{}.ckpt\".format(now)\n",
    "print(\"Best model saved as:\", best_model_path)\n",
    "print(\"Training time: %.8s seconds\" % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*-*-*-*-*-*-* Final test results *-*-*-*-*-*-*\n",
      "Best theta:\n",
      " [[ 2.06383395]\n",
      " [ 0.83894938]\n",
      " [ 0.11557128]\n",
      " [-0.2354842 ]\n",
      " [ 0.28999415]\n",
      " [-0.01035487]\n",
      " [ 0.00870804]\n",
      " [-0.86221409]\n",
      " [-0.83403569]]\n",
      "Final test set error:  0.537006\n"
     ]
    }
   ],
   "source": [
    "saver_restore = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver_restore.restore(sess, best_model_path) #restoring the previous model saved.\n",
    "    best_theta = theta.eval()\n",
    "    final_error = mse.eval(feed_dict={X: X_test_scaled_plus_bias, y: y_test})\n",
    "\n",
    "print(\"\\n*-*-*-*-*-*-* Final test results *-*-*-*-*-*-*\") \n",
    "print(\"Best theta:\\n\", best_theta)\n",
    "print(\"Final test set error: \", final_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusions and final remarks\n",
    "\n",
    "- By descreasing the size of the training set to 60% of the total when including the validation set, using the original learning_rate = 0.01 actually made the algorithm diverge. It was necessary to decrease the LR to 0.005.\n",
    "\n",
    "- It's a good practice to set 'stop_learning'=True because it decreases a lot the training time. Be careful setting the 'n_it' parameter though to not stop too early.\n",
    "\n",
    "- In logistic regression the squared error cost function is a bowl-shapped convex surface with respect to model's parameters. Hence, it does not  have flat regions like neural networks cost surface. In this case, early stopping is not very useful. Because we're using mini-batch gradient descent, after the convergence the training error will become to bounce around the minimum value. Moreover, because the linear regression model is a hyperplane, i.e. it has large bias, it seems strange that increasing the number of iterations, the model starts to overfit. It probably doesn't, because it doesn't have degrees of freedom for that; the linear function just bounce around the 'best' linear function. It's useful though to use early stopping to stop the training procedure automatically, avoiding iterating over the maximum number of steps defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
