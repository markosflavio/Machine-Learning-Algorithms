{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Neural Networks with Tensorflow\n",
    "<p class='lead'>\n",
    "Author: Oliveira, Markos F. B. G.<br />\n",
    "</p>\n",
    "\n",
    "# Description\n",
    "\n",
    "This document is based on Chapter 11 content of Geron's book *Hands-On Machine Learning with Scikit-Learn & TensorFlow*. It covers only the basics of the related topics and it's not meant to be a tutorial. Please, refer to the referenced book and scikit-learn/tensorflow online documentation for specific information.\n",
    "\n",
    "In paticular, this notebook provides an example of applying neural networks using Tensorflow on MNIST classification data.\n",
    "\n",
    "The algorithm that minimizes the cost function is a plain stochastic gradient descent with mean squared error. An automatic search for good hyperparameters is not implemented, nor ensemble methods are used. However, it employs a naive implementation of early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From TF dataset repository it's possible to load MNIST data. From the average values of mean and standard deviation it's probable that the digits in the test and validation sets came from same distribution of the training set. It would be possible to check if there are differences in the distributions of the digits between the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "Training data shape: inputs  (55000, 784) outputs (55000,)\n",
      "Validation data shape: inputs  (5000, 784) outputs (5000,)\n",
      "Test data shape: inputs  (10000, 784) outputs (10000,)\n",
      "\n",
      "Average value of the mean of all 784 features in training set: 0.130702\n",
      "Average value of the mean of all 784 features in validation set: 0.130223\n",
      "Average value of the mean of all 784 features in test set: 0.132516\n",
      "Average value of the standard deviation of all 784 features in training set: 0.193211\n",
      "Average value of the standard deviation of all 784 features in validation set: 0.192845\n",
      "Average value of the standard deviation of all 784 features in test set: 0.192418\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9EAAAHqCAYAAAADEs00AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNAAAAAAAniiiAQAAAADw9P/4ZLdXy0b8CgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d245d42710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#Downloading and extracting the data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "#Fetching the data\n",
    "X_train = mnist.train.images\n",
    "X_val = mnist.validation.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_val = mnist.validation.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")\n",
    "\n",
    "#Changing the outputs to 2d-arrays:\n",
    "#y_train = y_train.reshape(-1, 1)\n",
    "#y_val = y_val.reshape(-1, 1)\n",
    "#y_test = y_test.reshape(-1, 1)\n",
    "print('\\nTraining data shape: inputs ', X_train.shape, 'outputs', y_train.shape)\n",
    "print('Validation data shape: inputs ', X_val.shape, 'outputs', y_val.shape)\n",
    "print('Test data shape: inputs ', X_test.shape, 'outputs', y_test.shape)\n",
    "\n",
    "#Prining mean/std info\n",
    "print('\\nAverage value of the mean of all 784 features in training set:', np.mean(X_train.mean(axis=0)))\n",
    "print('Average value of the mean of all 784 features in validation set:', np.mean(X_val.mean(axis=0)))\n",
    "print('Average value of the mean of all 784 features in test set:', np.mean(X_test.mean(axis=0)))\n",
    "print('Average value of the standard deviation of all 784 features in training set:', np.mean(X_train.std(axis=0)))\n",
    "print('Average value of the standard deviation of all 784 features in validation set:', np.mean(X_val.std(axis=0)))\n",
    "print('Average value of the standard deviation of all 784 features in test set:', np.mean(X_test.std(axis=0)))\n",
    "\n",
    "#Plotting the histograms of the mean feature values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax = axes.ravel()\n",
    "ax[0].hist(X_train.mean(axis=0), bins=50, color='b', alpha=.5)\n",
    "ax[0].set_title('Training set average feature values')\n",
    "ax[1].hist(X_val.mean(axis=0), bins=50, color='b', alpha=.5)\n",
    "ax[1].set_title('Validation set average feature values')\n",
    "ax[2].hist(X_test.mean(axis=0), bins=50, color='b', alpha=.5)\n",
    "ax[2].set_title('Test set average feature values')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to see that the data is already normalized, all features have maximum value one and minimum value zero (minmax normalization). Some pixels have only zero values, such as pixel 0.\n",
    "In general, we apply the normalization in different datasets from info gained only in training set. In the case of minmax normalization, it's necessary to calculate the maximum and the minimum values (taken from training data). The training, validation and test datasets are then normalized, where each entry $x$ is updated to $x = \\frac{x-min(f_x)}{max(f_x)-min(f_x)}$, where $f_x$ is the feature where x entry belongs. Because the *min* and *max* values are taken from the training data, after this normalization its minimum and maximum value naturally becomes 0 and 1 respectively. However, applying this same values to the validaiton and test sets, their min and max values probably are not 0 and 1 after normalization.\n",
    "\n",
    "It can be seen however, that all sets have same min and max values of a pixel, which is strange. It's possible that these datasets were normalized independently, which according to Muller's book it's an improperly way to scale data (in page 135(pdf) it's presented an example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max and min values of pixel 0 in training set: 0.0/0.0\n",
      "Max and min values of a feature/pixel in training set: 1.0/0.0\n",
      "Max and min values of a feature/pixel in the validation set: 1.0/0.0\n",
      "Max and min values of a feature/pixel in the test set: 1.0/0.0\n"
     ]
    }
   ],
   "source": [
    "print('Max and min values of pixel 0 in training set: {}/{}'.format(np.max(X_train[:,0]), np.min(X_train[:,0])))\n",
    "print('Max and min values of a feature/pixel in training set: {}/{}'.format(np.max(X_train[:,300]), np.min(X_train[:,300])))\n",
    "print('Max and min values of a feature/pixel in the validation set: {}/{}'.format(np.max(X_val[:,300]), np.min(X_val[:,300])))\n",
    "print('Max and min values of a feature/pixel in the test set: {}/{}'.format(np.max(X_test[:,300]), np.min(X_test[:,300])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to train a neural network with training set and validation set together, even though it's common to have a validaiton set to perform early-stopping or to search for good hyperparameters, for example. It's not allowed however, mainly in this case where we have a standard database, to train including the test set. In a case where we need to implement a model in a real-world scenario though, we could include the test set, after its use in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape of training data:  (55000, 784)\n",
      "Outputs shape of training data:  (55000,)\n",
      "# inputs:  784\n",
      "# outputs:  10\n",
      "# instances:  55000\n"
     ]
    }
   ],
   "source": [
    "train_plus_val = False\n",
    "if train_plus_val:\n",
    "    X_train = np.concatenate((X_train, X_val), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_val), axis=0)\n",
    "\n",
    "#Storing some important values\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = (np.unique(y_train)).size\n",
    "m = X_train.shape[0]\n",
    "print('Inputs shape of training data: ', X_train.shape)\n",
    "print('Outputs shape of training data: ', y_train.shape)\n",
    "print('# inputs: ', n_inputs) # 28*28 = MNIST\n",
    "print('# outputs: ', n_outputs)\n",
    "print('# instances: ', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch creation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stochastic gradient descent it's necessary to provide stochastic small batches of examples so that the algorithm updates more often and eventually converges faster. The function returns a mini-batch when its called using two approaches.\n",
    "\n",
    "- In the 1st approach random indices from dataset are selected to be inside the mini-batch using a randomized seed, which is the number of the training iteration (epoch*n_batches + batch_index). In this case, one epoch does not pass through all the different examples.\n",
    "- In the 2nd approach a random permutation of the examples is passed and a slice of this permutation is used as mini-batch. In this case all the examples are used in one epoch, thus it's a better approach. A random permutation of the examples is necessary to be computed once per epoch oustide the iteration over the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size, idxs, Xs, ys, approach=2):\n",
    "    \"\"\"Returns the mini-batch (X, y) for a training step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : int\n",
    "        Number of epoch.\n",
    "    batch_index : int\n",
    "        Number of batch inside the epoch.\n",
    "    batch_size : int\n",
    "        Batch size.\n",
    "    idxs : list of int\n",
    "        Permutation of training set indices.\n",
    "    approach: int, optional\n",
    "        Approach used for chosing the mini-batches. \n",
    "    Returns\n",
    "    -------\n",
    "    X_batch : np.ndarray\n",
    "        Mini-batch of inputs.\n",
    "    y_batch : np.ndarray\n",
    "        Mini-batch of outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    if approach == 1:\n",
    "        # 1st approach: in this approach random indices from dataset are selected to be inside the mini-batch using a\n",
    "        #randomized seed, which is the number of the training iteration. In this case, one epoch probably does\n",
    "        #not have all the different examples.\n",
    "        rnd.seed(epoch * n_batches + batch_index)\n",
    "        indices = rnd.randint(m, size=batch_size)\n",
    "    elif approach == 2:\n",
    "        # 2nd approach: in this approach a random permutation of the examples is passed and a slice of this permutation\n",
    "        #is used as mini-batch.\n",
    "        indices = idxs[batch_index * batch_size: (batch_index + 1) * batch_size]\n",
    "    \n",
    "    X_batch = Xs[indices]\n",
    "    y_batch = ys[indices]\n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks layer creation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to have a function that manually creates the archictecure of a neural network. In this way, it's possible to create structures that are not fully connected. Below, it's implemented a function that creates a canonical feedforward network. The function creates each layer at a time, and not the fully structure. The weights are initialized using common method found in literature to speed up convergence and avoid saturation in early stages of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    \"\"\"Manually creates the layers of the neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input values of the layer (m_batch, n_inputs).\n",
    "    n_neurons : int\n",
    "        Number of neurons in the layer.\n",
    "    name : string\n",
    "        Scope name of the layer.\n",
    "    activation : string\n",
    "        Type of activation function.\n",
    "    Returns\n",
    "    -------\n",
    "    z : np.ndarray\n",
    "        The output of the layer.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs) # good strategy to initialize the NN's weights.\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev, seed = 0) # using truncated distribution we avoid\n",
    "        #values whose magnitude is more than 2 standard deviations (95%) from the mean, which is zero (these values are\n",
    "        #dropped and re-picked). It helps with convergence speed.\n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b #If m_match > 1 tf.matmul(X, W) is a matrix; b i summed to all of its columns.\n",
    "        \n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        elif activation==\"sigmoid\":\n",
    "            return tf.nn.sigmoid\n",
    "        elif activation==\"tanh\":\n",
    "            return tf.nn.tanh\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, some of the paramters related to training can be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SGD parameters:\n",
    "n_epochs = 500\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "#Early-stopping:\n",
    "early_stopping = True #enable early stopping.\n",
    "early_stopping_step = 1 #steps in the number of epochs stopping must be checked.\n",
    "min_val_error = float(\"inf\") #stores the best error so far.\n",
    "min_val_error_epoch = 0 #epoch when minimum validation error was found.\n",
    "n_it = 2*batch_size #maximum number of iterations without improvement in validation error.\n",
    "imp_counter = 0 #counts the umber of iterations there is no improvement in the validation error.\n",
    "stop_learning = False #enable stop learning after 'n_it' iterations of no improvement in validation error. If 'False', all\n",
    "#epochs are runned, but the model which least validation error is returned.\n",
    "\n",
    "#Monitoring:\n",
    "prt = True # enable printing training statistics.\n",
    "print_step = 100 # number of epochs to periodically print training statistics.\n",
    "\n",
    "#Logging:\n",
    "log = True # enable logging accuracy values.\n",
    "log_step = 1\n",
    "val_error_list = []\n",
    "train_error_list = []\n",
    "test_error_list = []\n",
    "\n",
    "#Neural networks layers:\n",
    "manual_layers = True #manually creates the layers using 'neural_layer' function.\n",
    "tf_batch = fetch_batch # use implemented TF function to load mini-batches.\n",
    "#for MNIST: tf_batch = mnist.train.next_batch(batch_size)\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "#Obs.: the archictecture of the layer must be constructed inside the 'CONSTRUCTION PHASE' of the nn graph.\n",
    "\n",
    "#Save/restore:\n",
    "restore = False # restore old model\n",
    "save_ckpt = False # save chackpoints\n",
    "saver_step = 100 # number of epochs to periodically save model's checkpoint.\n",
    "path_restore = '/tmp/finals/my_model_final.ckpt'\n",
    "\n",
    "#Tensorboard:\n",
    "tb = False # log training statistics for tensorboard.\n",
    "root_logdir = \"tf_logs\"\n",
    "tensorboard_step = 100 # number of epochs to periodically log statistics in tensorboard log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights initialization, activation functions and batch-normalization\n",
    "\n",
    "It's important to emphasize the parameters of the main funciton used in this code, tf.layers.dense():\n",
    "\n",
    "dense(inputs,\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=None,\n",
    "    bias_initializer=tf.zeros_initializer(),\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    trainable=True,\n",
    "    name=None,\n",
    "    reuse=None)\n",
    "        \n",
    "    inputs: Tensor input.\n",
    "    units: Integer or Long, dimensionality of the output space.\n",
    "    activation: Activation function (callable). Set it to None to maintain a linear activation.\n",
    "    use_bias: Boolean, whether the layer uses a bias.\n",
    "    kernel_initializer: Initializer function for the weight matrix.\n",
    "    bias_initializer: Initializer function for the bias.\n",
    "    kernel_regularizer: Regularizer function for the weight matrix.\n",
    "    bias_regularizer: Regularizer function for the bias.\n",
    "    activity_regularizer: Regularizer function for the output.\n",
    "    trainable: Boolean, if True also add variables to the graph collection GraphKeys.TRAINABLE_VARIABLES (see tf.Variable).\n",
    "    name: String, the name of the layer.\n",
    "    reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\n",
    "    \n",
    "Some remarkable notes are needed:\n",
    "\n",
    "* By changing the activation function of a layer, the best initialization strategy also changes; below we have some good initialization strategies for the most common functions.\n",
    "\n",
    "        * Sigmoid: Xavier initialization\n",
    "        Implementation:\n",
    "        #Using tf.contrib:\n",
    "        >> xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "        >> kernel_initializer = xavier_init\n",
    "        #Using manual code: (TF uses the uniform code below as xavier initialization)\n",
    "        >> low = -np.sqrt(6.0/(fan_in + fan_out)) # use 4 for sigmoid, 1 for tanh activation \n",
    "        >> high = np.sqrt(6.0/(fan_in + fan_out))\n",
    "        >> init = tf.random_uniform(shape=(n_inputs, n_neurons), minval=low, maxval=high, dtype=tf.float32, seed=0)\n",
    "        #or\n",
    "        >> stddev = np.sqrt(2.0/(fan_in + fan_out))\n",
    "        >> init = tf.truncated_normal(shape=(n_inputs, n_neurons), stddev=stddev, mean=0, dtype=tf.float32, seed=0)\n",
    "        \n",
    "        * Hyperbolic tangent\n",
    "        Implementation:\n",
    "        #Using manual code:\n",
    "        >> low = -4*np.sqrt(6.0/(fan_in + fan_out)) # use 4 for sigmoid, 1 for tanh activation \n",
    "        >> high = 4*np.sqrt(6.0/(fan_in + fan_out))\n",
    "        >> init = tf.random_uniform(shape=(n_inputs, n_neurons), minval=low, maxval=high, dtype=tf.float32, seed=0)\n",
    "        #or\n",
    "        >> stddev = 4*np.sqrt(2.0/(fan_in + fan_out))\n",
    "        >> init = tf.truncated_normal(shape=(n_inputs, n_neurons), stddev=stddev, mean=0, dtype=tf.float32, seed=0)\n",
    "        \n",
    "        * ReLu: He initialization\n",
    "        Implementation:\n",
    "        #Using tf.contrib:\n",
    "        >> he_init = tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\") #to consider both fan_int and fan_out.\n",
    "        >> kernel_initializer = he_init\n",
    "        #Using manual code:\n",
    "        >> low = -np.sqrt(2.0)*np.sqrt(6.0/(fan_in + fan_out)) # use 4 for sigmoid, 1 for tanh activation \n",
    "        >> high = np.sqrt(2.0)*np.sqrt(6.0/(fan_in + fan_out))\n",
    "        >> init = tf.random_uniform(shape=(n_inputs, n_neurons), minval=low, maxval=high, dtype=tf.float32, seed=0)\n",
    "        #or\n",
    "        >> stddev = np.sqrt(2.0)*np.sqrt(2.0/(fan_in + fan_out))\n",
    "        >> init = tf.truncated_normal(shape=(n_inputs, n_neurons), stddev=stddev, mean=0, dtype=tf.float32, seed=0)\n",
    "        \n",
    "* The ReLu function should be your primary choice of 'common' activation functions, it's faster to evaluate than sigmoids and hyp-tangent and does not saturatefor positive values. However, we may experience the *dying ReLUs* phenomena, wher most neurons are dead (output zero) and cannot back to life. To avoid this problem other ReLU activation variants exist:\n",
    "\n",
    "        * Leaky Relu: lr = max(alpha*z, z) \n",
    "        Alpha controls the slope of the function for z < 0 and it's tipically set to 0.01. But a recent paper *Empirical Evaluation of Rectified Activations in Convolution Network* found that a bigger leaky parameters of 0.2 (or 0.18) is consistently better.\n",
    "        Implementation:\n",
    "        >> def leaky_relu_function(z, alpha=0.2, name=None)\n",
    "        >>    return np.maximum(alpha*z, z, name=name)\n",
    "        >> hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu_function)\n",
    "        \n",
    "        * Randomized Leaky Relu (RReLU): rlr = max(alpha*z, z), alpha is randomly chosen.\n",
    "        - During training (for example, for each epoch) alpha is chosen randomly using a uniform distribution U(low, high), such as U(1/8, 1/3).\n",
    "        - During testing we chose an alpha of a_test = (low+high)/2\n",
    "        - RReLu act as a regularizer.\n",
    "        Implementation (training):\n",
    "        >> def random_leaky_relu_function(z, min, max, name=None)\n",
    "        >> assert((min<1) and (max<1))\n",
    "        >> alpha = tf.random_uniform(shape=(1, 1), minval=min, maxval=max, dtype=tf.float32, seed=0)\n",
    "        >>    return np.maximum(alpha*z, z, name=name)\n",
    "        >> hidden1 = tf.layers.dense(X, n_hidden1, activation=random_leaky_relu_function) \n",
    "        \n",
    "        * Exponential Linear Unit (ELU): if z < 0: f = alpha*(exp(z) - 1); if z >= 0: f = z.\n",
    "        - alpha controls where f approaches when z becomes -inf. It's usually set to one.\n",
    "        - has faster convergence rate than any other relu variants, however it's slower to compute.\n",
    "        Implementation:\n",
    "        >> act = tf.nn.elu\n",
    "        >> hidden1 = tf.layers.dense(X, n_hidden1, activation_fn=act) #alpha = 1\n",
    "        #or\n",
    "        >> def elu_function(z, alpha=1):\n",
    "        >>    return np.where(z<0, alpha*(np.exp(z)-1), z)\n",
    "        >> hidden1 = tf.layers.dense(X, n_hidden1, activation=elu_fuction)\n",
    "        \n",
    "* Trials order of the activation functions: ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic. If you're suffering from regularization problems, try RReLU; if evaluation should be fast use leaky ReLU instead of ELU. When using these ReLU variants, the initialization parameter should be He init.\n",
    "\n",
    "* Vanishing/exploding gradients problem still can occur during training even with the recommended choices of initialization and activation functions. To avoid this we could use batch-normalization.\n",
    "        * It makes the networks converge much faster.\n",
    "        * It makes the net more robust to the weight initialization.\n",
    "        * It makes possible to use larger learning rates.\n",
    "        * It acts as a regularizer, reducing the need of other techniques.\n",
    "        * It removes the need for normalizing the input data.\n",
    "        * It's slower to evaluate.\n",
    "        * The implementation of this technique is a bit complex, so it's passed to a new *Code* cell (read the comments to understand the batch-normalization implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.73 Test accuracy: 0.7354\n",
      "1 Train accuracy: 0.835 Test accuracy: 0.8128\n",
      "2 Train accuracy: 0.79 Test accuracy: 0.8413\n",
      "3 Train accuracy: 0.825 Test accuracy: 0.8582\n",
      "4 Train accuracy: 0.81 Test accuracy: 0.8663\n",
      "5 Train accuracy: 0.865 Test accuracy: 0.8743\n",
      "6 Train accuracy: 0.85 Test accuracy: 0.8812\n",
      "7 Train accuracy: 0.845 Test accuracy: 0.8869\n",
      "8 Train accuracy: 0.89 Test accuracy: 0.8918\n",
      "9 Train accuracy: 0.915 Test accuracy: 0.8947\n",
      "10 Train accuracy: 0.93 Test accuracy: 0.8973\n",
      "11 Train accuracy: 0.875 Test accuracy: 0.902\n",
      "12 Train accuracy: 0.905 Test accuracy: 0.9053\n",
      "13 Train accuracy: 0.865 Test accuracy: 0.9076\n",
      "14 Train accuracy: 0.91 Test accuracy: 0.9092\n",
      "15 Train accuracy: 0.955 Test accuracy: 0.9112\n",
      "16 Train accuracy: 0.89 Test accuracy: 0.912\n",
      "17 Train accuracy: 0.915 Test accuracy: 0.9163\n",
      "18 Train accuracy: 0.885 Test accuracy: 0.9182\n",
      "19 Train accuracy: 0.9 Test accuracy: 0.9181\n",
      "20 Train accuracy: 0.885 Test accuracy: 0.9213\n"
     ]
    }
   ],
   "source": [
    "# Example of implementation of batch_normalization using TF:\n",
    "from functools import partial\n",
    "run_example = True\n",
    "if run_example:\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    n_epochs = 20\n",
    "    batch_size = 100\n",
    "    n_inputs = 28 * 28  # MNIST\n",
    "    n_hidden1 = 300\n",
    "    n_hidden2 = 100\n",
    "    n_outputs = 10\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    #CONSTRUCTION PHASE\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "    is_training = tf.placeholder(tf.bool, shape=(), name='is_training')  #This will be used to tell the\n",
    "    #batch_norm() function whether it should use the current mini-batchs mean and standard deviation\n",
    "    #(during training) or the running averages that it keeps track of (during testing).\n",
    "\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        he_init = tf.contrib.layers.variance_scaling_initializer() #he initialization.\n",
    "        \n",
    "        #'partial' makes a new version of a function with one or more arguments already filled in.\n",
    "        my_batch_norm_layer = partial( \n",
    "                tf.layers.batch_normalization,\n",
    "                training=is_training,\n",
    "                momentum=0.9) #The algorithm uses exponential decay\n",
    "                #to compute the running averages, which is why it requires the decay parameters. Given a new value\n",
    "                #v, the running average vhat is updated through the equation vhat = vhat*decay + v*(1-decay). A good\n",
    "                #decay value is typically close to 1  for example, 0.9, 0.99, or 0.999 (you want more 9s for larger\n",
    "                #datasets and smaller mini-batches: which means that the batch contributes less to the overall value).\n",
    "                #In the code, decay is called 'momentum', that's not related to the momentum optimizer.\n",
    "        \n",
    "        my_dense_layer = partial(\n",
    "                tf.layers.dense,\n",
    "                kernel_initializer=he_init)\n",
    "        \n",
    "        hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "        bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "        hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "        bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "        logits_before_bn = my_dense_layer(bn2, n_outputs, activation=None, name=\"outputs\")\n",
    "        logits = my_batch_norm_layer(logits_before_bn)\n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) #the update operations needed by batch\n",
    "        #normalization are added to the UPDATE_OPS collection and you need to explicity run these operations during training.\n",
    "        #This extra update makes the  tf.layers.batch_normalization function updates\n",
    "        #the running averages right before it performs batch normalization during training.\n",
    "        \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "    with tf.name_scope(\"train\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "    with tf.name_scope(\"eval\"):\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #EXECUTION PHASE\n",
    "    \n",
    "    n_epochs = 20\n",
    "    batch_size = 200\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for epoch in range(n_epochs+1):\n",
    "            for iteration in range(len(mnist.test.labels)//batch_size):\n",
    "                X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "                sess.run([training_op, extra_update_ops], feed_dict={is_training: True, X: X_batch, y: y_batch})\n",
    "                #Note above that since we are using `tf.layers.batch_normalization()`, \n",
    "                #we need to explicitly run the extra update operations needed by batch normalization.\n",
    "            acc_train = accuracy.eval(feed_dict={is_training: False, X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={is_training: False, X: mnist.test.images, y: mnist.test.labels})\n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        #save_path = saver.save(sess, \"my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "* Below there is a implementation of the gradient clipping procedure, where we limit the gradients to avoid explosive grad problem. Most used in RNN.\n",
    "\n",
    "* optimizer.minimize() computes and also epply the gradients. For grad clipping, it's necessary to compute frist, apply yo the the gards the clipping and then apply.\n",
    "\n",
    "* Gradient clipping is not used when batch normalization is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False: #never runs\n",
    "    threshold = 1.0 #hyperparameter\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State of the Art Implementation of Neural Networks\n",
    "\n",
    "Effectively running the MNIST with optimized implementation using TF, including:\n",
    "- Specialized weight initialization.\n",
    "- Novel activation functions.\n",
    "- Batch-normalization.\n",
    "- Regularization.\n",
    "- Efficient Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training accuracy: 0.908033 Validation accuracy: 0.9216\n",
      "Epoch 100 Training accuracy: 0.999883 Validation accuracy: 0.9998\n",
      "Epoch 200 Training accuracy: 1.0 Validation accuracy: 1.0\n",
      "Epoch 300 Training accuracy: 1.0 Validation accuracy: 1.0\n",
      "Epoch 400 Training accuracy: 1.0 Validation accuracy: 1.0\n",
      "Epoch 500 Training accuracy: 1.0 Validation accuracy: 1.0\n",
      "Epoch 500 with minimum validation error 0.0.\n",
      "Best model saved as: ./tmp/best_model-20170814205245.ckpt\n",
      "Training time: 2603.517 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #restoring the default graph.\n",
    "tf.logging.set_verbosity(tf.logging.WARN) #supress TF logging messages when saving ckpt files.\n",
    "start_time = time.time()\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now) #relative path of tensorboard logs for a particular run (current time\n",
    "#is used so that each folder has different running stats, comparision between them can be made inside tensorboard).\n",
    "\n",
    "# TF CONSTRUCTION PHASE\n",
    "\n",
    "# 1- Creating variables, placeholders and constants.\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "# 2- Creating the operations.\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    if manual_layers:\n",
    "        hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "        hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "        logits = neuron_layer(hidden2, n_outputs, \"output\") # the final layer returns the logits only.\n",
    "        softmax = tf.nn.softmax(logits, name=\"softmax\")\n",
    "    else:\n",
    "        hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "        softmax = tf.nn.softmax(logits, name=\"softmax\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) #this computes the cross entropy based \n",
    "    #directly on the logits of the output layer: it expects integer labels (from 0 to n_outputs-1). This returns the cross-entropy\n",
    "    #scalar value for each instance.\n",
    "    #Use 'softmax_cross_entropy_with_logits()' if the labels are in the form of one-hot vectors.\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\") #computes the mean over the mini_batch.\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) #returns boolean == True if the output y is in the first k highest probabilities.  \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) #computes the accuracy.\n",
    "\n",
    "# 3- Node that initialize the variables.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 4- Creating the saver.\n",
    "saver_save = tf.train.Saver(max_to_keep=None) #save all variable values.\n",
    "if restore:\n",
    "    saver_restore = tf.train.Saver() #restore all variable values.\n",
    "    #saver_restore = tf.train.Saver({\"weights\": theta}) #restore only the old theta variable under the name of 'weights'.\n",
    "\n",
    "# 5- Tensorboard definitions:\n",
    "if tb:\n",
    "    accuracy_summary = tf.summary.scalar('ACC', accuracy) # Creates a node in the graph that will evaluate the ACC value\n",
    "    #and write it to a TensorBoard-compatible binary log string called a summary.\n",
    "    summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) # Creates a FileWriter that you will\n",
    "    #use to write summaries to logfiles in the log directory.\n",
    "\n",
    "# TF EXECUTION PHASE:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # To restore a model the construction phase must be identical than the one used to save it. \n",
    "    if restore:\n",
    "        saver_restore.restore(sess, path_restore)\n",
    "    else:\n",
    "        sess.run(init) # Initializing the variables.\n",
    "    \n",
    "    breaker = False\n",
    "    for epoch in range(n_epochs+1): # for each epoch..\n",
    "        \n",
    "        idXs = np.random.permutation(range(m))\n",
    "        \n",
    "        for batch_index in range(n_batches-1): # for each mini-batch..\n",
    "            X_batch, y_batch = tf_batch(epoch, batch_index, batch_size, idXs, X_train, y_train)\n",
    "            #Training step:\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        #Early-stopping (we could implement early stopping after each update (iteration), however, it would be too slow):\n",
    "        if (early_stopping) and (epoch % early_stopping_step == 0):\n",
    "            #val_error = loss.eval(feed_dict={X: X_val, y: y_val})\n",
    "            val_error = 1 - accuracy.eval(feed_dict={X: X_val, y: y_val})\n",
    "            imp_counter = imp_counter + 1 \n",
    "            if val_error <= min_val_error:\n",
    "                min_val_error = val_error\n",
    "                min_val_error_epoch = epoch\n",
    "                save_path = saver_save.save(sess, \"./tmp/best_model-{}.ckpt\".format(now))\n",
    "                imp_counter = 0\n",
    "            elif (stop_learning) and (imp_counter >= n_it):\n",
    "                breaker = True\n",
    "                break\n",
    "        if breaker:\n",
    "            break \n",
    "        \n",
    "        if (log) and (epoch % log_step == 0):\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "            acc_val = accuracy.eval(feed_dict={X: X_val, y: y_val})\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            val_error_list.append(acc_val)\n",
    "            train_error_list.append(acc_train)\n",
    "            test_error_list.append(acc_test)\n",
    "        if (prt) and (epoch % print_step == 0):\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "            acc_val = accuracy.eval(feed_dict={X: X_val, y: y_val})\n",
    "            print(\"Epoch\", epoch, \"Training accuracy:\", acc_train, \"Validation accuracy:\", acc_val)\n",
    "        if (tb) and (epoch % tensorboard_step == 0):\n",
    "            summary_str = accuracy_summary.eval(feed_dict={X: X_train, y: y_train}) # This will output a summary that\n",
    "            #you can then write to the events file using the file_writer. Here is the updated code:\n",
    "            step = epoch * n_batches + batch_index\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "        if (save_ckpt) and (epoch % saver_step == 0):\n",
    "            save_path = saver_save.save(sess, \"./tmp/my_model-{}.ckpt\".format(epoch))\n",
    "            \n",
    "    if not (early_stopping):\n",
    "        save_path = saver_save.save(sess, \"./tmp/best_model-{}.ckpt\".format(now))\n",
    "\n",
    "# Flushing and closing FileWriter.          \n",
    "if tb:                            \n",
    "    summary_writer.flush()\n",
    "    summary_writer.close() \n",
    "\n",
    "if early_stopping:\n",
    "    print('Epoch {} with minimum validation error {:.4}.'.format(min_val_error_epoch, min_val_error))\n",
    "best_model_path = \"./tmp/best_model-{}.ckpt\".format(now)\n",
    "print(\"Best model saved as:\", best_model_path)\n",
    "print(\"Training time: %.8s seconds\" % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logged CPU/GPU times:\n",
    "- NN with MNIST: 1185.27s (~19m)/\n",
    "- NN with MNIST (early stopping): 2083.05s (~35m)/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in the test set of the best model found in the validation set:  0.9798\n"
     ]
    }
   ],
   "source": [
    "saver_restore = tf.train.Saver() \n",
    "with tf.Session() as sess:\n",
    "    saver_restore.restore(sess, best_model_path)\n",
    "    X_test = mnist.test.images  # remember to transform this set if necessary.\n",
    "    Z = logits.eval(feed_dict={X: X_test})\n",
    "    y_prob = softmax.eval(feed_dict={X: X_test})\n",
    "    y_pred = np.argmax(Z, axis=1) #or np.argmax(y_prob, axis=1)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy in the test set of the best model found in the validation set: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot below it's possible to see the performance of the model. Note that one possible, and probably good criteria to stop learning is to monitor the traiing set accuracy. If it became too high (near 1.00, considering a tolerance) we can safely stop learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAFkCAYAAAB8RXKEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e9c00aa198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(val_error_list[:100], label='Validation accuracy')\n",
    "plt.plot(train_error_list[:100], label='Train accuracy')\n",
    "plt.plot(test_error_list[:100], label='Test accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Retraining the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to improve network performance it would be train using the training and validation sets until the epoch with best validation error. Experiments must be done to check the variance with respect to the knowledge obtained in the validation set, particularly the epoch.\n",
    "\n",
    "Note that one must not use the test set to choose the least error, this is the purpose of the validation set only. Any choice made on the test set imediatly contaminates it, it looses its reliability of being a proxy of the generalization error.\n",
    "\n",
    "It's not clear if we should renormalize the data after combining the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train, X_val), axis=0)\n",
    "y_train = np.concatenate((y_train, y_val), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To trust in the result gained in the validation set, the network should be trained in the same way, with same parameters. For example, it's desirable to have the same initial weights of the first run. See however, that the mini-batches are unequal considering the 2nd approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved as: ./tmp/best_model-20170814184904.ckpt\n",
      "Training time: 358.2318 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #restoring the default graph.\n",
    "tf.logging.set_verbosity(tf.logging.WARN) #supress TF logging messages when saving ckpt files.\n",
    "start_time = time.time()\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now) #relative path of tensorboard logs for a particular run (current time\n",
    "#is used so that each folder has different running stats, comparision between them can be made inside tensorboard).\n",
    "\n",
    "# TF CONSTRUCTION PHASE\n",
    "\n",
    "# 1- Creating variables, placeholders and constants.\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "# 2- Creating the operations.\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    if manual_layers:\n",
    "        hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "        hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "        logits = neuron_layer(hidden2, n_outputs, \"output\") # the final layer returns the logits only.\n",
    "        softmax = tf.nn.softmax(logits, name=\"softmax\")\n",
    "    else:\n",
    "        hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\") #Default 'activation' == None\n",
    "        softmax = tf.nn.softmax(logits, name=\"softmax\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) #this computes the cross entropy based \n",
    "    #directly on the logits of the output layer: it expects integer labels (from 0 to n_outputs-1). This returns the cross-entropy\n",
    "    #scalar value for each instance.\n",
    "    #Use 'softmax_cross_entropy_with_logits()' if the labels are in the form of one-hot vectors.\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\") #computes the mean over the mini_batch.\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) #returns boolean == True if the output y is in the first k highest probabilities.  \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) #computes the accuracy.\n",
    "\n",
    "# 3- Node that initialize the variables.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 4- Creating the saver.\n",
    "saver_save = tf.train.Saver(max_to_keep=None) #save all variable values.\n",
    "\n",
    "\n",
    "# TF EXECUTION PHASE:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init) # Initializing the variables.\n",
    "    \n",
    "    for epoch in range(min_val_error_epoch+50): # for each epoch..\n",
    "        \n",
    "        idXs = np.random.permutation(range(m))\n",
    "        \n",
    "        for batch_index in range(n_batches-1): # for each mini-batch..\n",
    "            X_batch, y_batch = tf_batch(epoch, batch_index, batch_size, idXs, X_train, y_train)\n",
    "            #Training step:\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "    save_path = saver_save.save(sess, \"./tmp/final_model-{}.ckpt\".format(now))\n",
    "\n",
    "final_model_path = \"./tmp/final_model-{}.ckpt\".format(now)\n",
    "print(\"Final model saved as:\", best_model_path)\n",
    "print(\"Training time: %.8s seconds\" % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in the test set of the final model retrained in the training and validation sets:  0.978\n"
     ]
    }
   ],
   "source": [
    "saver_restore = tf.train.Saver() \n",
    "with tf.Session() as sess:\n",
    "    saver_restore.restore(sess, final_model_path)\n",
    "    X_test = mnist.test.images  # remember to transform this set if necessary.\n",
    "    Z = logits.eval(feed_dict={X: X_test})\n",
    "    y_prob = softmax.eval(feed_dict={X: X_test})\n",
    "    y_pred = np.argmax(Z, axis=1) #or np.argmax(y_prob, axis=1)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy in the test set of the final model retrained in the training and validation sets: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
