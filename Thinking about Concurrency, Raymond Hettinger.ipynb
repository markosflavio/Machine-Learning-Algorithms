{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking about Concurrency, Raymond Hettinger\n",
    "\n",
    "Markos Flavio B. G. O.\n",
    "\n",
    "This notebook highlights the keynotes and the walkthrough of the presentation of Raymond Hettinger on PyCon 2016, 'Thinking about Concurrency'.\n",
    "\n",
    "The original content is available at:\n",
    "    - https://www.youtube.com/watch?v=Bv25Dwe84g0\n",
    "    - https://pybay.com/site_media/slides/raymond2017-keynote/index.html\n",
    "    - https://www.youtube.com/watch?v=9zinZmE3Ogk&t=122s\n",
    "    - https://pybay.com/site_media/slides/raymond2017-keynote/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "We'll walk through two examples of threading and multiprocessing to illustrate rules and best practices for taking advantage of concurrency.\n",
    "\n",
    "### Why Concurrency?\n",
    "\n",
    "Concurrency is about taking advantage of the computer power available to you.\n",
    "\n",
    "1.\tImprove perceived responsiveness (slower wait time for different requests)\n",
    "2.\tImprove speed: mainly when we use multiple processes (multiple cores) - you through more clock cycles to the problem; remember, sometimes, concurrency can make your code go slower.\n",
    "3.\tBecause that is how the real world works. Pretty much of all of the concurrency primitives that we use in a computer are also used in project management. For example, let’s say two independent teams are working on a project of a satellite. The two teams need to finish their own job before the two parts can be assembled together, this is a simple thread join. During the satellite construction, it needs to be put in a “shake room” that effectively shakes the satellite to see if it falls apart before the launch. You can have two satellites in the shake room at the same time, you need mutually exclusion, you need locks.\n",
    "In this context, when we want to computer systems to model the real world, it makes sense to think about concurrency as the standard way of approaching a problem, instead of viewing it as a last resort.\n",
    "\n",
    "### Martelli Model of Scaleability\n",
    "\n",
    "There’re three kinds of programs:\n",
    "\n",
    "- 1 core: Single thread and single process: they take advantage of one core.\n",
    "- 2-8 cores: Multiple threads and multiple processes.\n",
    "- 9+ cores: Distributed processing.\n",
    "\n",
    "Martelli’s observation: As time goes on, the second category becomes less common and relevant and single cores become more powerful (current cores are thousand time faster than 80’s cores; thus, the range of problems that can be solved using one core, is much larger than before). For example, we can use Tensor Flow to train a very good hand written or voice recognition model in a feasible amount of time just in a single core. However, if we're dealing with a big data context where big datasets grow ever larger (if the data that you’re talking about fits in your machine, that’s not big data), we must rely on distributed processing.\n",
    "The consequence of this is that the second category (that refers to single machiune swith multiple cores) becomes less relevant over time, because the other two paradigms fits better to current problems. However, current machines still have more than one core embed on it and it’s really unpleasant to use only 1/8th to the power of programs that are built on top of that machines. Now, suppose we're dealing with a problem in an 8-core machine that requires seven cores. We can use this machine to attack the problem using multiple threads and multiple processes. However, the problem complexity is so close to the computational limit of the machine that if the probelm grows 20% we'll not be able to use the current setup to solve it anymore. Thus, if you have a problem in this range (that's solvable from 2 to 8 cores) you happend to be be lucky for just that point in time. And, it doesn't wirth the effort to be working in this space, because you just happend to temporarily be in it. **However, even if you're in the distributed processing scenario, you want to take advgantage of all cores of all parallel machines; thus the second category, whose approach relies on multi-threading and multi-processing is still an area of interesting even if the problem is going to be bigger than that.**\n",
    "\n",
    "### Global Interpreter Lock\n",
    "\n",
    "CPython has a single lock for its internal shared global state. The payoff of GIL is that you don't pay the performance costs that individual locks (necessary if GIL didn't existed to make Python work properly) requires. The unfortunate effect of the GIL is that no more than one thread can run at a time. However, we have some ways of solving that problem: if you can't get free threading in one python, why don't run eght Python codes in parallel, each with their own thread to get the advantage of all cores. Or, we can combine multi-threading and multi-processing, etc (there're a number of ways to ignore the GIL).\n",
    "\n",
    "For I/O bound applications, the GIL doesn’t present much of an issue (for Web Servers and whatnot, multi-threading works fine). However, for CPU bound applications, using threading makes the application speed worse. Accordingly, that drives us to multi-processing to gain more CPU cycles.\n",
    "\n",
    "### Threads vs Processes\n",
    "\n",
    "“Your weakness is your strength and your strength is your weakness”.\n",
    "\n",
    "The strength of threads is shared state (communication is quickly). The weakness of threads is shared state (managing race conditions: every multi-threading have a race condition (so, you have to design \"manual\" locks so the threads \"don't step on each other\", because if it didn’t, you didn’t really needed multi-threading).\n",
    "\n",
    "The strength of processes is their independence from one another (they don’t have a shared state). The weakness of processes is lack of communication (hence, the need for IPC to move objects between the process and other additional overhead: in the multiprocessing  module, some issues are abstracted away from the user and they’re hidden; however, if you’re moving a lot of data, it’s very important to be aware pickling the data through IPC and such tremendous amount of overhead; if you’re using multi-process in a thread pool you need to be aware of the good news, a shared state, and bad news, a potential for race conditions and a GIL that keeps you for using multiple cores).\n",
    "\n",
    "### Threads vs Async\n",
    "\n",
    "#### Threads\n",
    "Threads switch preemptively (the thread manager (not the current thread itself) decides for you when to switch tasks). This is convenient because you don’t need to add explicit code to cause a task switch.\n",
    "\n",
    "The cost of this convenience is that you have to assume a switch can happen at any time. For example,  let's say two variables must be consistent to each other, having equal values. However, if you update one and get preempted the other one might not be updated and you'll leave the system in an incoherent state. In fact, that's reason for the GIL to exist, so as you execute your Python program, the global state is constantly updating which task is runing, which line number are you on, etc.\n",
    "\n",
    "Accordingly, critical sections (very important code sections) have to be guarded with locks, or queues, or other type of synchronization tools. And the idea is that if two things are happpened together, I aquire a lock which says \"nobody else should be runing right now\", do the critical section and then, release the lock and let other tasks to run. Reasoning about critical sections and developing locks correctly in large multi-threading programs is insanely difficult, even though it's possible to get it correct. In this context, a lock is essentially a flag (it, per se, doesn't lock anything) that signals something is locked. And, if someone checks that signal and receive the \"locking information\", it will not touch on the locked resource, but only if it's checked. If another thread forgets to aquire the lock. Thus, even if the large multi-threading program is  correct, it won't necessarily stay correct over time because tiny little adjustments to the code can cause it to become incorrect in a way that is hard to see during code reviews.   \n",
    "\n",
    "The limit on threads is total CPU power (number of CPU cycles) minus the cost of task switches and synchronization overhead. For example, every time a task switch and every time you acquire and release locks they eat some CPU cycles. That's why one single lock (GIL) is constless than several. Thus, threading never adds power to the system, if fact it consumes some power. Now, threading is useful when you can spend some CPU power for solving the problem, if you can't spend such power (as in CPU bound applications) you must rely on a different approach: asynchronous (Async).\n",
    "\n",
    "#### Async\n",
    "Async switches cooperatively (you don't get interrupted in arbitrarily times, when doing your work and when you get to a good stopping point and your state is consistent you go back to the async manager and let other task to run), so you do need to add explicit code “yield” or “await” to cause a task switch (that's a little disadvantage that worth to go thorugh given its advantages).\n",
    "\n",
    "Now you control when task switches occur, so locks and other synchronization are no longer needed. \n",
    "\n",
    "Also, the cost task switches is very low. Calling a pure Python function has more overhead than restarting a generator or awaitable, i.e. the cost of calling a function in Python is more expensive than a task switch in Async. This means that async is very cheap. Async uses generators under the hood that store all of their states (stack frame) and when we call the generator again, it has all the information it needs to continue to work. Otherwise, when calling a funciton, it needs to build up the state, build a new stack frame on every call.  \n",
    "\n",
    "We can say that for CPU-bound applications, because Async is very cheap (by far) than threading, it makes sense to use it, because it gives us more CPU clocks, that would be lost in threading due task switches and synchronization overhead. While we can run hundreads of threads in parallel we can run thousands or tens of thousands async tasks per second.\n",
    "\n",
    "Also, because Asyncs doesn't have locks ut's a lot easier to get the code correct. You just change tasks when they are consistent and you don't have to worry about arbitrarily interruptions.\n",
    "\n",
    "In return, you’ll need a non-blocking version of just about everything you do; you can't just read from a file anymore using a standard Python funciton for it - you need to use a Async compatible function. Accordingly, the async world has a huge ecosystem of support tools. This increases the learning curve, which is even higher than the one of threading.\n",
    "\n",
    "#### Comparison\n",
    "- Async maximizes CPU utilization because it has less overhead than threads.\n",
    "- Threading typically works with existing code and tools as long as locks are added around critical sections. Thus, if you are using in a project a lot of existing code and existing libraries and want to take advantages of concurrency, you should use threading because changing all that code to fit Async may be unfeasible. \n",
    "- For complex systems, async is much easier to get right than threads with locks.\n",
    "- Threads require very little tooling (locks and queues).\n",
    "- Async needs a great deal of tooling (futures, event loops, and non-blocking versions of just about everything). \"We only know half of Python, the other half of it, is Async.\" Async is continually to grow, reaching its coverage on the entire language; anything in the standard Python language that doesn't fit with Async is about to get its own version of it that is asynchronous. \n",
    "\n",
    "Raymond Gettinger here says that that even though with async Python will roughly double its size, Async it's the future for concurrency in Python community. That's because threads are hard to develop and are very expensive. Also, as the ecosystem of Async is getting bigger (which it is), it becomes easier to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threading Example\n",
    "#### Scripting style\n",
    "\n",
    "Start with working code that is clear, simple, and runs top to bottom (create global variables). This is easy to develop and test incrementally (type a few lines, run itm, type a few lines, run it..). This style is very quick, it let's you concretely see your results, it let's you to tst as you go and you can reliablly knock out code even with little experience in programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An important note abnout threading: Get your app tested and debugged in a singled threaded mode first before you start threading. Threading NEVER makes debugging easier (it always includes another layer of complexity).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating and printing a counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up\n",
      "The count is 1\n",
      "---------------\n",
      "The count is 2\n",
      "---------------\n",
      "The count is 3\n",
      "---------------\n",
      "The count is 4\n",
      "---------------\n",
      "The count is 5\n",
      "---------------\n",
      "The count is 6\n",
      "---------------\n",
      "The count is 7\n",
      "---------------\n",
      "The count is 8\n",
      "---------------\n",
      "The count is 9\n",
      "---------------\n",
      "The count is 10\n",
      "---------------\n",
      "Finishing up\n"
     ]
    }
   ],
   "source": [
    "counter = 0 # global variable\n",
    "\n",
    "print('Starting up')\n",
    "for i in range(10):\n",
    "    counter += 1\n",
    "    print('The count is %d' % counter)\n",
    "    print('---------------')\n",
    "print('Finishing up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function style\n",
    "A next step in development is to factor re-usable code into functions. Below, the reusable component is the *worker* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up\n",
      "The count is 1\n",
      "---------------\n",
      "The count is 2\n",
      "---------------\n",
      "The count is 3\n",
      "---------------\n",
      "The count is 4\n",
      "---------------\n",
      "The count is 5\n",
      "---------------\n",
      "The count is 6\n",
      "---------------\n",
      "The count is 7\n",
      "---------------\n",
      "The count is 8\n",
      "---------------\n",
      "The count is 9\n",
      "---------------\n",
      "The count is 10\n",
      "---------------\n",
      "Finishing up\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    global counter\n",
    "\n",
    "    counter += 1\n",
    "    print('The count is %d' % counter)\n",
    "    print('---------------')\n",
    "\n",
    "print('Starting up')\n",
    "for i in range(10):\n",
    "    worker()\n",
    "print('Finishing up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-threading is easy!\n",
    "It is just a matter of launching a few worker threads. Note below that instead of callinf the *worker* function directly, we launch it in a thread so that we have a main thread running (the *for* loop) and the *worker^* thread.\n",
    "\n",
    "This code is very similar to the original functional one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up\n",
      "The count is 1\n",
      "---------------\n",
      "The count is 2\n",
      "---------------\n",
      "The count is 3\n",
      "---------------\n",
      "The count is 4\n",
      "---------------\n",
      "The count is 5\n",
      "---------------\n",
      "The count is 6\n",
      "---------------\n",
      "The count is 7\n",
      "---------------\n",
      "The count is 8\n",
      "---------------\n",
      "The count is 9\n",
      "---------------\n",
      "The count is 10\n",
      "---------------\n",
      "Finishing up\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    global counter\n",
    "\n",
    "    counter += 1\n",
    "    print('The count is %d' % counter)\n",
    "    print('---------------')\n",
    "\n",
    "print('Starting up')\n",
    "for i in range(10):\n",
    "    threading.Thread(target=worker).start()\n",
    "print('Finishing up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above is broken (we'll see later why), however it passed in the test of running the code: no errors were returned. In this context.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can you spot the race conditions?\n",
    "\n",
    "*A race condition occurs when two or more threads can access shared data and they try to change it at the same time. Because the thread scheduling algorithm can swap between threads at any time, you don't know the order in which the threads will attempt to access the shared data.*\n",
    "\n",
    "Most people spot the “counter increment” race condition: we can have one thread looking at the value of *counter*, another thread looking at the same value of *counter*, both will increment the same base value, both of them will write out the same incremented values and we have two workers run, but our *counter* is incremented one time.\n",
    "\n",
    "Even though, there's a race condition here; now, because this happens so fast, it's unlikely that a task switch is going to happen in between the read and the write of the counter inside the worker function, so you can possibly run this code billions of times and never have this problem show up, even after deployment. \n",
    "\n",
    "Also, there's another race condition happening here, that's a little more hidden, which is “print function” race condition. We have the main thread printing the 'Finishing up' string and the other threads printing the counters. It's possible to have the main thread string printed before some of the counter's printing.\n",
    "\n",
    "Why didn’t testing reveal the flaws?\n",
    "\n",
    "**Note: Testing cannot prove the absence of errors. It is still useful, don’t rely on it in the context of multi-threading. Many interest racing conditions don’t reveal themselves in test environments (they manifest themselves under load, abnormal conditions, in ways there're really hard to reproduce, creating Heisenberg's bugs (from Heisenberg's uncertainty principle): if you're looking at the bug, the act of looking at it, causes the object to changes its behavior towards no longer a buggy. In particular, if we run a code like this through a dubugger, we'll never see the race condition, because the debugger interferes with all the races.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, there's a technique that can be used to amplify the race existing conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fuzzing\n",
    "Fuzzing is a technique for amplifying race conditions, so they become visible. The technique is based on putting the program to sleep in between to each command of the operation a random period of time; we put it between every code step, becuse a thread switch can happen any time. It isn't a perfect technique, but it's a decent way of detecting race conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up\n",
      "The count is 2\n",
      "---------------\n",
      "The count is 2The count is 2\n",
      "\n",
      "---------------\n",
      "---------------The count is 3The count is 3\n",
      "\n",
      "\n",
      "---------------\n",
      "---------------\n",
      "The count is 4The count is 5The count is 5\n",
      "---------------Finishing up\n",
      "The count is 6\n",
      "\n",
      "The count is 6\n",
      "\n",
      "\n",
      "---------------\n",
      "------------------------------\n",
      "---------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import threading, time, random\n",
    "\n",
    "##########################################################################################\n",
    "# Fuzzing is a technique for amplifying race condition\n",
    "# errors to make them more visible\n",
    "\n",
    "FUZZ = True\n",
    "\n",
    "def fuzz():\n",
    "    if FUZZ:\n",
    "        # random sleep between 0 and 1 second\n",
    "        time.sleep(random.random()) \n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    global counter\n",
    "\n",
    "    fuzz()\n",
    "    oldcnt = counter\n",
    "    fuzz()\n",
    "    counter = oldcnt + 1\n",
    "    fuzz()\n",
    "    print('The count is %d' % counter, end='')\n",
    "    fuzz()\n",
    "    print() # prints a new line\n",
    "    fuzz()\n",
    "    print('---------------', end='')\n",
    "    fuzz()\n",
    "    print() # prints a new line\n",
    "    fuzz()\n",
    "\n",
    "print('Starting up')\n",
    "fuzz()\n",
    "for i in range(10):\n",
    "    threading.Thread(target=worker).start()\n",
    "    fuzz()\n",
    "print('Finishing up')\n",
    "fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kind of output could have happened in production runs of the original code, under load or other unreproducible conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is limited to relatively small blocks of code and is imperfect in that is can’t prove the absence of errors. Still, fuzzed tests do reveal the presence of errors (like above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Approach: More Careful Threading with Queues\n",
    "Interestingly, the rules for threading are just for computing and programming. The physical world is full of concurrency as well. Many of these techniques has physical analogs that are useful for managing people and projects.\n",
    "\n",
    "One careful way of dealing with multi-hreading is to use atomic message queues instead of locks. Locks are great if you're developing an operating system, however, for anything higher-level than that, i.e. real applications, people don't think in terms of the complexity of locks. Thus, we need a more high-level primitives: we'd like to have something that we can emulate to, atomic message queues.\n",
    "\n",
    "We deal with a atomic message queues because we indeed have a queue object in Python, but anything that you can communicate atomically would work, such as email accounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raymond Rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RR 1000**\n",
    "\n",
    "ALL shared resources SHALL be run in EXACTLY ONE thread. ALL communication with that thread SHALL be done using an atomic message queue: typically the Queue module, email, message queues like RabbitMQ or ZeroMQ, interesting you can communicate via a database as well.\n",
    "\n",
    "Resources that need this technique: global variables, user input, output devices, files, sockets, etc.\n",
    "\n",
    "Some resources that already have locks inside (thread-safe): logging module, decimal module (thread local variables), databases (reader locks and writer locks), email (this is an atomic message queue). Pretty much everything else should presumed to be non-atomic in nature and should be wrapped in *its own* thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RR 1001**\n",
    "\n",
    "One category of sequencing problems is to make sure that step A and step B happen sequentially. The solution is to put both in the same thread where all actions proceed sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RR 1002**\n",
    "\n",
    "To implement a “barrier” that waits for parallel threads to complete, just join() all of the threads. Remember: a terrible time to use a join() (or to wait to some task to be finished) is when we're not sure if the thread is going to finish; an infinit loop. A thread that has an infinite loop that never finishes is called a *deamon* thread. *Deamonize a thread is to mark the thread and say that it's never going to finish, so don't wait on it!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RR 1003**\n",
    "\n",
    "You can’t wait on daemon threads to complete (they are infinite loops). Instead, you join() on the message queue itself, rather on the thread itself. It waits until all the requested tasks are marked as being done.\n",
    "\n",
    "Let's see an example where workers (threads) are people and links of communication are email accounts. You send an email to a worker and says \"I'd like you to do task A\". Now, you should not wait to the worker to read of all the emails because this didn't mean the worker finished task A, it only means that the worker received and started the task. Thus, it's insufficient to wait for the message queue to be empty. Instead, I need to send the following email to my worker: \"Do task A and answer me when you're done\". And that's a tradicityonal way of doing in Python: using two message queues. An existing method built on the message queue itself is a method called *task done*. The workerretrieves a message, do the work and mark the task as being done, so that someone can join, not the thread, but join the email (message) queue. Thus:\n",
    "- when you have a non-deamon threads, you join the thread.\n",
    "- when you have a deamon threds, you join the email queue for talking to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RR 1004**\n",
    "\n",
    "Sometimes you need a global variable to communicate between functions. Global variables work great for this purpose in a single threaded program. In multi-threaded code, it mutable global state is a disaster. The better solution is to use a threading.local() that is global WITHIN a thread but not without; that mans each tread will have its wn copy of that global variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RR 1005**\n",
    "Never try to kill a thread from something external to that thread. You never know if that thread is holding a lock; if it is, every other thread that's going to wait on that lock, instantly become dead locks. Python doesn’t provide a direct mechanism (API) for kill threads externally (because you shouldn't do it); however, you can do it using ctypes, but that is a recipe for a deadlock.\n",
    "\n",
    "Killing a thread is a conceptually terrible idea (not an implementation problem), because if we kill a thread that's holding a lock, our program will be in a deadlock state.\n",
    "\n",
    "*Note: a deadlock is a situation in which two computer programs sharing the same resource are effectively preventing each other from accessing the resource, resulting in both programs ceasing to function.*\n",
    "\n",
    "We can still kill a thread in Python, calling the operating system and killing manually. Another way is to using the *ctypes* module to reach into the thread and kill it: *However, just because a language allows you to dop something, it doesn't mean you should do something\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying all the rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note below that:\n",
    "- The counter and print (two apparent cources of race conditions) were isolated in their own threads.\n",
    "- The counter and print threads are both deamon, they both have infinitue loops inside them (*while True..*), and they are \"marked\" as such.\n",
    "- Because both threads are deamon, we should join the message queue instead of the threads themselves.\n",
    "- The counter thread, that has exclusive rights to updating the counter varible, performs:\n",
    "    1. It constantly checks if it receives any new message through the *get()* method.\n",
    "    2. It increments the counter.\n",
    "    3. It sends an atomic message to the print queue saying \"print this message\". The print might run in its own speed, but it also has an atomic message queue that sequences all of the actions coming in.\n",
    "    4. After done, it call the *task_done()* method, so that later we can wait on the queue itself to see if its done.\n",
    "- The print thread, that has exclusive rights to call the print keyword, performs:\n",
    "    1. Looking if it receives any new message through the *get()* method; otherwise it sleeps.\n",
    "    2. Prints every line of the message queue. In fact, there's a race condition here. However, it's not a problem, because this thread has exclusive rights of using the print condition and thus, it will only win the race (it has only one competitor).\n",
    "    3. After done, it call the *task_done()* method.\n",
    "- The worker job is to send a message to the conter queue. Remember, we cannot increment the counter directly, we can only communicate between queues.\n",
    "- In the main loop we start the worker threads, because they are not deamon threads. Thus, after starting such threads we join them. It doesn't mean that the increments and printing are done, because all the workers job is to send an email (message); so our guaranteee is that we wait until all the ten messages are sent.\n",
    "- After joining the workers, we join the counter and print queues. We do not wait until these queues are empty, because we're dealing will deamon threads; we wait on ther queues untill both of them says \"for every message that I've received, I have a 'task done'\".\n",
    "- When the final message to the print queue saying \"Finishing up\", we're not guaranteed that all the 10 print messages are in fact, already printed. However, we've guaranteed that such queue already received 11 previous messages, starting with the \"Starting up\" message. Because the queue is FIFO, we've guaranteed that the last message to be printed is the last in the queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the smallest possible solution to deal with this problem correclty considering multi-threading. Look how a simple problem has its complexity increased in a multi-threading context. We must fear and respect for multi-threading. However, if we apply the rules above, we can systematically work through any problem and guaranteee that the code is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up\n",
      "The count is 1\n",
      "---------------\n",
      "The count is 2\n",
      "---------------\n",
      "The count is 3\n",
      "---------------\n",
      "The count is 4\n",
      "---------------\n",
      "The count is 5\n",
      "---------------\n",
      "The count is 6\n",
      "---------------\n",
      "The count is 7\n",
      "---------------\n",
      "The count is 8\n",
      "---------------\n",
      "The count is 9\n",
      "---------------\n",
      "The count is 10\n",
      "---------------\n",
      "Finishing up\n"
     ]
    }
   ],
   "source": [
    "import queue #FIFO\n",
    "##########################################################################################\n",
    "# Fuzzing is a technique for amplifying race condition\n",
    "#errors to make them more visible\n",
    "\n",
    "FUZZ = True # we set this to true for debugging purposes\n",
    "\n",
    "def fuzz():\n",
    "    if FUZZ:\n",
    "        time.sleep(random.random())\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "counter = 0\n",
    "\n",
    "counter_queue = queue.Queue()\n",
    "\n",
    "def counter_manager():\n",
    "    'I have EXCLUSIVE rights to update the counter variable'\n",
    "    global counter\n",
    "\n",
    "    while True:\n",
    "        increment = counter_queue.get()\n",
    "        fuzz()\n",
    "        oldcnt = counter\n",
    "        fuzz()\n",
    "        counter = oldcnt + increment\n",
    "        fuzz()\n",
    "        print_queue.put([\n",
    "            'The count is %d' % counter,\n",
    "            '---------------'])\n",
    "        fuzz()\n",
    "        counter_queue.task_done()\n",
    "\n",
    "t = threading.Thread(target=counter_manager)\n",
    "t.daemon = True\n",
    "t.start()\n",
    "del t\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "print_queue = queue.Queue()\n",
    "\n",
    "def print_manager():\n",
    "    'I have EXCLUSIVE rights to call the \"print\" keyword'\n",
    "    while True:\n",
    "        job = print_queue.get()\n",
    "        fuzz()\n",
    "        for line in job:\n",
    "            print(line, end='')\n",
    "            fuzz()\n",
    "            print()\n",
    "            fuzz()\n",
    "        print_queue.task_done()\n",
    "        fuzz()\n",
    "\n",
    "t = threading.Thread(target=print_manager)\n",
    "t.daemon = True\n",
    "t.start()\n",
    "del t\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    counter_queue.put(1)\n",
    "    fuzz()\n",
    "\n",
    "print_queue.put(['Starting up'])\n",
    "fuzz()\n",
    "\n",
    "# main loop:\n",
    "worker_threads = []\n",
    "for i in range(10):\n",
    "    t = threading.Thread(target=worker)\n",
    "    worker_threads.append(t)\n",
    "    t.start()\n",
    "    fuzz()\n",
    "for t in worker_threads:\n",
    "    fuzz()\n",
    "    t.join()\n",
    "\n",
    "counter_queue.join()\n",
    "fuzz()\n",
    "print_queue.put(['Finishing up'])\n",
    "fuzz()\n",
    "print_queue.join()\n",
    "fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaned-up code without fuzzing\n",
    "For production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up\n",
      "The count is 1\n",
      "---------------\n",
      "The count is 2\n",
      "---------------\n",
      "The count is 3\n",
      "---------------\n",
      "The count is 4\n",
      "---------------\n",
      "The count is 5\n",
      "---------------\n",
      "The count is 6\n",
      "---------------\n",
      "The count is 7\n",
      "---------------\n",
      "The count is 8\n",
      "---------------\n",
      "The count is 9\n",
      "---------------\n",
      "The count is 10\n",
      "---------------\n",
      "Finishing up\n"
     ]
    }
   ],
   "source": [
    "import threading, queue\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "counter = 0\n",
    "\n",
    "counter_queue = queue.Queue()\n",
    "\n",
    "def counter_manager():\n",
    "    'I have EXCLUSIVE rights to update the counter variable'\n",
    "    global counter\n",
    "\n",
    "    while True:\n",
    "        increment = counter_queue.get()\n",
    "        # RR1: if want to thing to happen sequantially: but that things in\n",
    "        # the same thread. Below, after the increment we send a request a print to be done\n",
    "        # (by putting a new massage in the print queue). That guarantess that updating and printing\n",
    "        # happen sequentially.\n",
    "        \n",
    "        counter += increment\n",
    "        print_queue.put([\n",
    "            'The count is %d' % counter,\n",
    "            '---------------'])\n",
    "        counter_queue.task_done()\n",
    "\n",
    "# the counter is isolated in its own deamon thread: counter manager\n",
    "# other thread is never going to update the counter, they just send a\n",
    "# message to the counter thread that an update must be done. Then, the updates\n",
    "# contained in the atomic message queue are performed sequentially, one \n",
    "# at a time. We're isolating resources: RR2.\n",
    "t = threading.Thread(target=counter_manager)\n",
    "t.daemon = True\n",
    "t.start()\n",
    "del t # The del keyword is used to delete objects.\n",
    "# In Python everything is an object, so the del keyword\n",
    "# can also be used to delete variables, lists, or parts of a list etc.\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "print_queue = queue.Queue()\n",
    "\n",
    "# The print has its own deamon thread and we comunicate to that through\n",
    "# its message queue. \n",
    "def print_manager():\n",
    "    'I have EXCLUSIVE rights to call the \"print\" keyword'\n",
    "    while True:\n",
    "        job = print_queue.get()\n",
    "        for line in job:\n",
    "            print(line)\n",
    "        print_queue.task_done()\n",
    "\n",
    "t = threading.Thread(target=print_manager)\n",
    "t.daemon = True\n",
    "t.start()\n",
    "del t # The del keyword is used to delete objects.\n",
    "# In Python everything is an object, so the del keyword\n",
    "# can also be used to delete variables, lists, or parts of a list etc.\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    counter_queue.put(1)\n",
    "\n",
    "print_queue.put(['Starting up'])\n",
    "# launching the workers\n",
    "worker_threads = []\n",
    "for i in range(10):\n",
    "    t = threading.Thread(target=worker)\n",
    "    worker_threads.append(t)\n",
    "    t.start()\n",
    "# waiting for the workers to be done (RR3)\n",
    "for t in worker_threads:\n",
    "    t.join()\n",
    "# waiting for the deamon threads to be done (joining the queue\n",
    "# instead of the thread) \n",
    "counter_queue.join()\n",
    "print_queue.put(['Finishing up'])\n",
    "print_queue.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code with just seven lives of code, becomes a code with about 60 lines. Indeed, it requires a great effort to get multi-threading correct: **It's far more complicated and time demanding than creating the application itself**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd Approach: Careful Threading with locks\n",
    "This isn't a recommended way of handling multi-threads due the complexity of the primitive, even though the code itself might be easier sometimes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up\n",
      "The count is 1\n",
      "---------------\n",
      "The count is 2\n",
      "---------------\n",
      "The count is 3\n",
      "---------------\n",
      "The count is 4\n",
      "---------------\n",
      "The count is 5\n",
      "---------------\n",
      "The count is 6\n",
      "---------------\n",
      "The count is 7\n",
      "---------------\n",
      "The count is 8\n",
      "---------------\n",
      "The count is 9\n",
      "---------------\n",
      "The count is 10\n",
      "---------------\n",
      "Finishing up\n"
     ]
    }
   ],
   "source": [
    "import threading, time, random\n",
    "\n",
    "##########################################################################################\n",
    "# Fuzzing is a technique for amplifying race condition\n",
    "# errors to make them more visible\n",
    "\n",
    "FUZZ = True\n",
    "\n",
    "def fuzz():\n",
    "    if FUZZ:\n",
    "        time.sleep(random.random())\n",
    "\n",
    "###########################################################################################\n",
    "\n",
    "counter_lock = threading.Lock()\n",
    "printer_lock = threading.Lock()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    global counter\n",
    "    with counter_lock:\n",
    "        oldcnt = counter\n",
    "        fuzz()\n",
    "        counter = oldcnt + 1\n",
    "        fuzz()\n",
    "        with printer_lock:\n",
    "            print('The count is %d' % counter, end='')\n",
    "            fuzz()\n",
    "            print()\n",
    "            fuzz()\n",
    "            print('---------------', end='')\n",
    "            fuzz()\n",
    "            print()\n",
    "        fuzz()\n",
    "\n",
    "with printer_lock:\n",
    "    print('Starting up', end='')\n",
    "    fuzz()\n",
    "    print()\n",
    "fuzz()\n",
    "\n",
    "worker_threads = []\n",
    "for i in range(10):\n",
    "    t = threading.Thread(target=worker)\n",
    "    worker_threads.append(t)\n",
    "    t.start()\n",
    "    fuzz()\n",
    "for t in worker_threads:\n",
    "    t.join()\n",
    "    fuzz()\n",
    "\n",
    "with printer_lock:\n",
    "    print('Finishing up', end='')\n",
    "    fuzz()\n",
    "    print()\n",
    "\n",
    "fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without fuzzing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting up\n",
      "The count is 1\n",
      "---------------\n",
      "The count is 2\n",
      "---------------\n",
      "The count is 3\n",
      "---------------\n",
      "The count is 4\n",
      "---------------\n",
      "The count is 5\n",
      "---------------\n",
      "The count is 6\n",
      "---------------\n",
      "The count is 7\n",
      "---------------\n",
      "The count is 8\n",
      "---------------\n",
      "The count is 9\n",
      "---------------\n",
      "The count is 10\n",
      "---------------\n",
      "Finishing up\n"
     ]
    }
   ],
   "source": [
    "\n",
    "counter_lock = threading.Lock()\n",
    "printer_lock = threading.Lock()\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def worker():\n",
    "    'My job is to increment the counter and print the current count'\n",
    "    global counter\n",
    "    with counter_lock:\n",
    "        counter += 1\n",
    "        with printer_lock:\n",
    "            print('The count is %d' % counter)\n",
    "            print('---------------')\n",
    "\n",
    "with printer_lock:\n",
    "    print('Starting up')\n",
    "\n",
    "worker_threads = []\n",
    "for i in range(10):\n",
    "    t = threading.Thread(target=worker)\n",
    "    worker_threads.append(t)\n",
    "    t.start()\n",
    "for t in worker_threads:\n",
    "    t.join()\n",
    "\n",
    "with printer_lock:\n",
    "    print('Finishing up')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lock approach (preliminary) results:\n",
    "\n",
    "- It is perfect!\n",
    "- It is beautiful.\n",
    "- It is simpler than using queues.\n",
    "\n",
    "However.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on Locks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RR 1005**\n",
    "\n",
    "Locks don’t lock anything. They are just flags and can be ignored. It is a cooperative tool, not an enforced tool.\n",
    "\n",
    "Look at the *print* function. Is it locked? Somewhere else in the code I can add a *print* without a lock; so locks don't really lock anything in the multi-threading code; someone can access a global variable anywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RR 1006**\n",
    "\n",
    "In general, locks should be considered a low level primitive that is difficult to reason about in non-trivial examples. For more complex applications, you’re almost always better off with using atomic message queues.\n",
    "\n",
    "**RR 1007**\n",
    "\n",
    "The more locks you are acquire at one time, the more you lose the advantages of concurrency.\n",
    "\n",
    "The code above is correct, but it's slower than the original, it takes no advantage of the concurrency at all, it's fully sequential and thus, deterministic (it runs in same way every time). Putting too many locks on a problem, may transform the solution to a sequential one, where we undone all the efects (positive and negative) of multi-hreading. This version works exactly the same as the original seven lines version.\n",
    "\n",
    "Multi-htreading approaches that relies on locks to avoid race conditions that run deterministically during several tests, means that there's no multi-htreading and the code runs sequentially due locking.\n",
    "\n",
    "Also, if you write a correct program using locks that as well designed and is not actually sequential, you're doing something that is almost isomorphic to using the atomic message queues. In fact, the *queue* module is a very thin layer built on top of a deck that has locks on it, the same locks that you've written anyway. However, wether we're using message queues or locks, it doesn't take away the fact that we have a GIL that threading does nothing for you in CPython. So, you would want to switch to the multi-processing module to take advantage of multiple cores. Multi-processing have all the same concepts, including atomic message queues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the queue module already have locks in it, so we do not need to work with the low level tool and mess with the synchronization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dining Philosophers\n",
    "\n",
    "*In computer science, the dining philosophers problem is an example problem often used in concurrent algorithm design to illustrate synchronization issues and techniques for resolving them.*\n",
    "\n",
    "*Five silent philosophers sit at a round table with bowls of spaghetti. Forks are placed between each pair of adjacent philosophers.*\n",
    "\n",
    "*Each philosopher must alternately think and eat. However, a philosopher can only eat spaghetti when they have both left and right forks. Each fork can be held by only one philosopher and so a philosopher can use the fork only if it is not being used by another philosopher. After an individual philosopher finishes eating, they need to put down both forks so that the forks become available to others. A philosopher can take the fork on their right or the one on their left as they become available, but cannot start eating before getting both forks.*\n",
    "\n",
    "*Eating is not limited by the remaining amounts of spaghetti or stomach space; an infinite supply and an infinite demand are assumed.*\n",
    "\n",
    "*The problem is how to design a discipline of behavior (a concurrent algorithm) such that no philosopher will starve; i.e., each can forever continue to alternate between eating and thinking, assuming that no philosopher can know when others may want to eat or think.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rules given above help you reliably create multi-threaded code when the underlying data flow is a DAG (directed acyclic graph).\n",
    "\n",
    "When the control flow or data flow is circular, the problem can be much harder (but still solvable). At that point, more formal design and verification techniques are warranted. Otherwise, it can be quite difficult in complex applications to avoid deadlock, have thread starvation, or to have unfair solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-processing Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrypting style\n",
    "Displaying the homepage sizes for multiple website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.yahoo.com/ 439618\n",
      "http://www.cnn.com 1133779\n",
      "http://www.python.org 49694\n",
      "http://www.jython.org 10286\n",
      "http://www.pypy.org 5539\n",
      "http://www.perl.org 12397\n",
      "http://www.cisco.com 93949\n",
      "http://www.facebook.com 127619\n",
      "http://www.twitter.com 385266\n",
      "http://arstechnica.com/ 76107\n",
      "http://www.reuters.com/ 182283\n",
      "http://abcnews.go.com/ 199595\n",
      "http://www.cnbc.com/ 1678958\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "sites = [\n",
    "    'https://www.yahoo.com/',\n",
    "    'http://www.cnn.com',\n",
    "    'http://www.python.org',\n",
    "    'http://www.jython.org',\n",
    "    'http://www.pypy.org',\n",
    "    'http://www.perl.org',\n",
    "    'http://www.cisco.com',\n",
    "    'http://www.facebook.com',\n",
    "    'http://www.twitter.com',\n",
    "    #'http://www.macrumors.com/',\n",
    "    'http://arstechnica.com/',\n",
    "    'http://www.reuters.com/',\n",
    "    'http://abcnews.go.com/',\n",
    "    'http://www.cnbc.com/',\n",
    "]\n",
    "\n",
    "for url in sites:\n",
    "    with urllib.request.urlopen(url) as u:\n",
    "        page = u.read()\n",
    "        print(url, len(page))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fucntion sytle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('https://www.yahoo.com/', 438839)\n",
      "('http://www.cnn.com', 1133779)\n",
      "('http://www.python.org', 49694)\n",
      "('http://www.jython.org', 10286)\n",
      "('http://www.pypy.org', 5539)\n",
      "('http://www.perl.org', 12397)\n",
      "('http://www.cisco.com', 93949)\n",
      "('http://www.facebook.com', 127767)\n",
      "('http://www.twitter.com', 385265)\n",
      "('http://arstechnica.com/', 76107)\n",
      "('http://www.reuters.com/', 182298)\n",
      "('http://abcnews.go.com/', 199595)\n",
      "('http://www.cnbc.com/', 1678949)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sites = [\n",
    "    'https://www.yahoo.com/',\n",
    "    'http://www.cnn.com',\n",
    "    'http://www.python.org',\n",
    "    'http://www.jython.org',\n",
    "    'http://www.pypy.org',\n",
    "    'http://www.perl.org',\n",
    "    'http://www.cisco.com',\n",
    "    'http://www.facebook.com',\n",
    "    'http://www.twitter.com',\n",
    "    #'http://www.macrumors.com/',\n",
    "    'http://arstechnica.com/',\n",
    "    'http://www.reuters.com/',\n",
    "    'http://abcnews.go.com/',\n",
    "    'http://www.cnbc.com/',\n",
    "]\n",
    "\n",
    "def sitesize(url):\n",
    "    ''' Determine the size of a website '''\n",
    "    with urllib.request.urlopen(url) as u:\n",
    "        page = u.read()\n",
    "        return url, len(page)\n",
    "\n",
    "for result in map(sitesize, sites):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to make your code running correctly in a single thread before putting it to work with concurrency. In this case, the easiest way is to use the *map* function, which is sequential. Interestingly, the *map* function makes the transition easier to the multi-processing approach:\n",
    "** a good development strategy is to use map to test the code in a single process and single thread mode before switching to multi-processing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is parallelizeable?\n",
    "A key pattern of thinking is to divide the world into to “lawn mowing” (total time is roughly (that's a little overhead due coordination of workers) divided by the number of workers) versus “baby making” (we have to wait 9 months for the work to be done, regardless the number of workers) – identifying tasks that are significantly parallelizeable versus those that are intrinsically serial. Most problems are in a slighly scale in between these two extremes of \"baby making\" and \"lawn mowing\" and this is quantitied by something called Amdahl’s Law.\n",
    "\n",
    "Amdahl’s Law (according to wikipedia):\n",
    "*Amdahl’s law is often used in parallel computing to predict the theoretical speedup when using multiple processors. For example, if a program needs 20 hours using a single processor core, and a particular part of the program which takes one hour to execute cannot be parallelized, while the remaining 19 hours (p = 0.95) of execution time can be parallelized, then regardless of how many processors are devoted to a parallelized execution of this program, the minimum execution time cannot be less than that critical one hour. Hence, the theoretical speedup is limited to at most 20 times (1/(1 − p) = 20). For this reason parallel computing is relevant only for a low number of processors and very parallelizable programs.*\n",
    "\n",
    "Detailed example:\n",
    "\n",
    "In the parallizeable approach, both steps assigned with 1) can be done together. However, the first one, is an expensive step, measured in miliseconds and the second one is cheap, being measured in microseconds. The following steps must that both previous steps be done to start working. Thus, in this case, there's really no value at parallelizing this task.\n",
    "\n",
    "Now, the HTTP request (step 2)) it's interesting. One requesting from home, you tipically has only one connection out to the internet. But, when requesting from your work, you've a bundle of fiber. In this scenario, you can send many several HTTP range requests (each request covering a portion of the whole page), getting the data that come back in parallel and then reassebmle it. Here, we've a great deal of potential to speed our code due parallelization. This particular approach is called Chanel bonding (an example of appication is provided here: http://stackoverflow.com/questions/8293687/sample-http-range-request-session).\n",
    "\n",
    "Also, we don't have to wait to count all characters, we may count one packet at a time.\n",
    "\n",
    "In other words, the three commands below are indeed parallelizable. However, it probably don't worth the time of doing it, unless there're a lot of data comming out of the requests. A better approach for this problem is to treat these three commands as one single step and do many different url's in parallel, which is much easier (check the code under 'Pools of processes' section below). **Remember: when you do multi-processing, do it in a highest-level possible so that you get the maximum pay-off.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sitesize(url):\n",
    "    ''' Determine the size of a website\n",
    "\n",
    "    This is non-parallizeable (the following steps are all sequential actions):\n",
    "    * UDP DNS request for the url\n",
    "    * UDP DNS response\n",
    "    * Acquire socket from the OS\n",
    "    * TCP Connection (three-way handshake):  SYN, ACK, SYN/ACK\n",
    "    * Send HTTP Request for the root resource\n",
    "    * Wait for the TCP response which is broken-up\n",
    "      into packets (we must join them!).\n",
    "    * Count the characters of the webpage\n",
    "    \n",
    "    However, some steps can be parallizeable. This problem would be classified\n",
    "    as 95% as \"baby making\" (instrinsicaly serial) and 5% as \"lawn mowing\". Thus, it\n",
    "    doesn't worth the trouble to get this program parallelizable.\n",
    "    \n",
    "    For example, steps marked as 1) can be done together.\n",
    "    Do ten times in parallel:\n",
    "        1) DNS lookup (UDP request and resp)\n",
    "        1) Acquire the socket\n",
    "        2) Send HTTP range requests\n",
    "        3) The sections comes back in parallel\n",
    "           across different pieces of fiber.\n",
    "        4) Count the characters for a single\n",
    "           block as received.\n",
    "    Add up the 10 results!\n",
    "\n",
    "    '''\n",
    "    u = urllib.request.urlopen(url)\n",
    "    page = u.read()\n",
    "    return url, len(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pools of processes\n",
    "\n",
    "We can get less than 9 months for one baby but we can get multiple n babies in less than n*9 months, using pools of processes. The total time here it will be the speed of making the slowest baby.\n",
    "\n",
    "Note below the multi-processing version of the single-thread map version. We had only one line changing: we've only changed the call to the *map* function to the *imap_unordered* function.\n",
    "\n",
    "Note that the mapping funciton returns its input (url) together with the output, which may appear unnecessary. However, if you design your mapping functions that way, you can use the imap_unordered to improve responsiveness and you don't need to care about the order of the results.\n",
    "\n",
    "**The use of imap_unordered is made possible by designing the function to return both its argument and its result as a tuple.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://www.perl.org', 12397)\n",
      "('http://www.jython.org', 10286)\n",
      "('http://www.cisco.com', 93949)\n",
      "('http://www.reuters.com/', 182219)\n",
      "('http://www.python.org', 49694)\n",
      "('http://www.cnn.com', 1133779)\n",
      "('http://www.pypy.org', 5539)\n",
      "('http://www.twitter.com', 385266)\n",
      "('http://www.facebook.com', 127925)\n",
      "('http://www.cnbc.com/', 1679107)\n",
      "('http://www.cnbc.com/', 1679107)\n",
      "('http://abcnews.go.com/', 199946)\n",
      "('http://arstechnica.com/', 76194)\n",
      "('https://www.yahoo.com/', 439968)\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "# from multiprocessing.pool import Pool\n",
    "\n",
    "sites = [\n",
    "    'https://www.yahoo.com/',\n",
    "    'http://www.cnn.com',\n",
    "    'http://www.python.org',\n",
    "    'http://www.jython.org',\n",
    "    'http://www.pypy.org',\n",
    "    'http://www.perl.org',\n",
    "    'http://www.cisco.com',\n",
    "    'http://www.facebook.com',\n",
    "    'http://www.twitter.com',\n",
    "    #'http://www.macrumors.com/',\n",
    "    'http://arstechnica.com/',\n",
    "    'http://www.reuters.com/',\n",
    "    'http://abcnews.go.com/',\n",
    "    'http://www.cnbc.com/',\n",
    "    'http://www.cnbc.com/',\n",
    "]\n",
    "\n",
    "def sitesize(url):\n",
    "    ''' Determine the size of a website '''\n",
    "    with urllib.request.urlopen(url) as u:\n",
    "        page = u.read()\n",
    "        return url, len(page)\n",
    "\n",
    "pool = Pool(10)\n",
    "for result in pool.imap_unordered(sitesize, sites):\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hazards of thin channel communication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a traveler from US that travels to Paris for a lunch and then, travels to Rome for dinner and to Porto for sleepping. What's wrong with this plan? There're too many trips back and forth between works. This example shows up three useful tricks to deal with multi-processing, that when violated, often generate poor solutions:\n",
    "1. Don’t make too many trips back and forth.\n",
    "2. Do significant work on each trip.\n",
    "3. Don’t send or receive a lot of data (send in summary queries send out and summary results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have SQL examples of violations of the rules we've say, in a xontext where we want to compute the sumation of the salary of all employees in the same department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-22-0d277807fb73>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-22-0d277807fb73>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    for employee, dept, salary in c.execute('SELECT employee, dept, salary FROM Employee')\u001b[0m\n\u001b[1;37m                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "###########################################################################################\n",
    "# Too many trips back and forth\n",
    "# Running a query to each department.\n",
    "summary = dict()\n",
    "for dept in c.execute('SELECT DISTINCT dept FROM Employee'):\n",
    "    c.execute('SELECT SUM(salary) FROM Employee')\n",
    "    summary[dept] = c.fetchone()[0]\n",
    "\n",
    "###########################################################################################\n",
    "# Bringing too much back and not doing enough work while you are there\n",
    "# Below, we load an entire employee's database and compute the summary\n",
    "summary = collections.Counter()\n",
    "for employee, dept, salary in c.execute('SELECT employee, dept, salary FROM Employee')\n",
    "    summary[dept] += salary\n",
    "\n",
    "###########################################################################################\n",
    "# Right way is one trip with where a lot of work gets done and only a summary result in returned\n",
    "\n",
    "summary = dict(execute('SELECT dept, SUM(salary) FROM Employee GROUPBY dept'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try any of the two first approaches, the performance will be poor. Even though, this is obvious for databases, when working with multi-processing people tend to forget these, making these same mistakes.\n",
    "\n",
    "- In the first case, we're using a range request and returning summary data (only the length of the page), but we're making far too many trips.\n",
    "- In the second case we're returning one line at a time (to many trips back and forth).\n",
    "- In the third case, we're returning the entire webpage instead of its link (to much data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too many trips back and forth\n",
    "# If the input iterable to map is very large, it suggests you're making too many trips\n",
    "\n",
    "def sitesize(url, start):\n",
    "    req = urllib.request.Request()\n",
    "    req.add_header('Range:%d-%d' % (start, start+1000))\n",
    "    u = urllib.request.urlopen(url, req)\n",
    "    block = u.read()\n",
    "    return url, len(block)\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "# Not doing enough work relative to the travel time\n",
    "# Once you get to a process, be sure to do enough work to make the trip worthwhile\n",
    "\n",
    "def sitesize(url, results):\n",
    "    with urllib.request.urlopen(url) as u:\n",
    "        while True:\n",
    "            line = u.readline()\n",
    "            results.put((url, len(line)))\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "# Taking too much with you or bringing too much back\n",
    "def sitesize(url):\n",
    "    u = urllib.request.urlopen(url)\n",
    "    page = u.read()\n",
    "    return url, page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Multi-processing notes\n",
    "\n",
    "Never run a multi-processing example from within an IDE that runs in the same process as the code you are developing. Otherwise, the forking step will fork the IDE itself as well as your code.\n",
    "\n",
    "When partitioning into subtasks, a common challenge is how to handle data at the boundaries of the partition.\n",
    "\n",
    "Setting the number of processes is a bit of an art. If the code is CPU bound, the number of cores times two is a reasonable starting point. If the code is IO bound, the number of cores can be much higher. Experimentation is the key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Threading and Forking\n",
    "\n",
    "Never combine threading and forking (fork is an operation whereby a process creates a copy of itself), it will produce deadlocks every time.\n",
    "\n",
    "The general rule is if you will combine threading and forking, which you shouldn't, “thread after you fork, not before”. Otherwise, the locks used by the thread executor will get duplicated across processes. If one of those processes dies while it has the lock, all of the other processes using that lock will deadlock.\n",
    "\n",
    "In other words. If you create the thread first, these threads will inevitably create locks. As soons as you fork, these locks will be share accross different processes. If you kill one of the processes that has a lock, all of the other processes will become deadlock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting 0\n",
      "starting 1\n",
      "starting 2\n",
      "starting 3\n",
      "starting 4starting 5\n",
      "\n",
      "starting 6\n",
      "starting 7\n",
      "starting 8\n",
      "starting 9starting 10\n",
      "\n",
      "starting 11\n",
      "starting 12\n",
      "starting 13\n",
      "starting 14\n",
      "starting 15\n",
      "finished 8\n",
      "finished 6\n",
      "finished 4\n",
      "finished 10\n",
      "finished 12\n",
      "finished 14\n",
      "finished 15\n",
      "finished 7\n",
      "finished 1\n",
      "finished 2\n",
      "finished 9\n",
      "finished 0\n",
      "finished 5\n",
      "finished 3\n",
      "finished 13\n",
      "finished 11\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# coding:utf8\n",
    "\n",
    "import sys\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def run(arg):\n",
    "    print(\"starting %s\" % arg)\n",
    "    p = multiprocessing.Process(target=print, args=(\"running\", arg))\n",
    "    p.start()\n",
    "    p.join()\n",
    "    print(\"finished %s\" % arg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n = 16\n",
    "    tests = range(n)\n",
    "    with ThreadPoolExecutor(n) as pool:\n",
    "        for r in pool.map(run, tests):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Example\n",
    "The following example covers how to build a performant non-blocking server from scratch, how to isolate the user’s business logic in callbacks, how to write the callback logic in-line with generators, and how to schedule timed events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server up, running, and waiting for call on localhost 9600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-193fed4b2a9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[0mreactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'localhost'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-193fed4b2a9b>\u001b[0m in \u001b[0;36mreactor\u001b[1;34m(host, port)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;31m# Serve existing clients BUT only if they already have data ready\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mready_to_read\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msessions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mready_to_read\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import socket, time, types, select\n",
    "from collections import namedtuple\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "######### Reactor ####################################################################\n",
    "\n",
    "ScheduledEvent = namedtuple('ScheduleEvent', ['event_time', 'task'])\n",
    "Session = namedtuple('Session', ['address', 'file'])\n",
    "\n",
    "events = []                   # heap with events prioritized by earliest time\n",
    "sessions = {}                 # { csocket : Session(address, file)}\n",
    "callback = {}                 # { csocket : callback(client, line) }\n",
    "generators = {}               # { csocket : inline callback generator}\n",
    "\n",
    "def reactor(host='localhost', port=9600):\n",
    "    'Main event loop that triggers the appropriate business logic callbacks'\n",
    "    s = socket.socket()\n",
    "    s.bind((host, port))\n",
    "    s.listen(5)\n",
    "    s.setblocking(0)          # Make asynchronous.  Never wait on a client socket.\n",
    "    sessions[s] = None\n",
    "    print('Server up, running, and waiting for call on %s %s' % (host, port))\n",
    "    try:\n",
    "        while True:\n",
    "            # Serve existing clients BUT only if they already have data ready\n",
    "            ready_to_read, _, _ = select.select(sessions, [], [], 0.1)\n",
    "            for c in ready_to_read:\n",
    "                if c is s:\n",
    "                    c, a = c.accept()\n",
    "                    connect(c, a)\n",
    "                    continue\n",
    "                line = sessions[c].file.readline()\n",
    "                if line:\n",
    "                    callback[c](c, line.rstrip())\n",
    "                else:\n",
    "                    disconnect(c)\n",
    "\n",
    "            # Run events scheduled at the appropriate event time\n",
    "            while events and events[0].event_time <= time.monotonic():\n",
    "                event = heappop(events)\n",
    "                event.task()\n",
    "    finally:\n",
    "        s.close()\n",
    "\n",
    "def connect(c, a):\n",
    "    'Reactor logic for new connections'\n",
    "    sessions[c] = Session(a, c.makefile())\n",
    "    on_connect(c)                            # call into user's business logic\n",
    "\n",
    "def disconnect(c):\n",
    "    'Reactor logic to end sessions'\n",
    "    on_disconnect(c)                         # call into user's business logic\n",
    "    sessions[c].file.close()\n",
    "    c.close()\n",
    "    del sessions[c]\n",
    "    del callback[c]\n",
    "\n",
    "def add_task(event_time, task):\n",
    "    'Helper function to schedule one-time tasks at specific time'\n",
    "    heappush(events, ScheduledEvent(event_time, task))\n",
    "\n",
    "def call_later(delay, task):\n",
    "    'Helper function to schedule one-time tasks after a given delay'\n",
    "    add_task(time.time() + delay, task)\n",
    "\n",
    "def call_periodic(delay, interval, task):\n",
    "    'Helper function to schedule recurring tasks'\n",
    "    def inner():\n",
    "        task()\n",
    "        call_later(interval, inner)\n",
    "    call_later(delay, inner)\n",
    "\n",
    "\n",
    "def on_connect(c):\n",
    "        g = nbcaser(c)          # 'g' is a coroutine\n",
    "        generators[c] = g       # generators -> awaitables\n",
    "        callback[c] = g.send(None)  # we do this to advance `nbcaser` coroutine\n",
    "                                    # to yield through the 'readline' coroutine\n",
    "                                    # which will sleep on its 'yield' expression\n",
    "\n",
    "def on_disconnect(c):\n",
    "        g = generators.pop(c)\n",
    "        g.close()\n",
    "\n",
    "@types.coroutine\n",
    "def readline(c):\n",
    "    'A non-blocking readline to use with two-way generators'\n",
    "    def inner(c, line):\n",
    "        g = generators[c]\n",
    "        try:\n",
    "            callback[c] = g.send(line)  # `g.send(line)` will resume the `yield inner` point\n",
    "        except StopIteration:\n",
    "            disconnect(c)\n",
    "    line = yield inner\n",
    "    return line\n",
    "\n",
    "def sleep(c, delay):\n",
    "    'A non-blocking sleep to use with two-way generators'\n",
    "    def inner():\n",
    "        g = generators[c]\n",
    "        callback[c] = next(g)\n",
    "    call_later(delay, inner)\n",
    "    return lambda *args: callback[c]\n",
    "\n",
    "\n",
    "######### User's Business Logic ######################################################\n",
    "\n",
    "def announcement():\n",
    "    print('The event loop is still running at:', time.ctime())\n",
    "\n",
    "call_periodic(delay=1, interval=15, task=announcement)\n",
    "\n",
    "async def nbcaser(c):\n",
    "    upper, title = 'upper', 'title'\n",
    "    mode = upper\n",
    "    print(\"Received connection from\", sessions[c].address)\n",
    "    try:\n",
    "        c.sendall(b'<welcome: starting in upper case mode>\\n')\n",
    "        while 1:\n",
    "            line = await readline(c)\n",
    "            if line == 'quit':\n",
    "                c.sendall(b'quit\\r\\n')\n",
    "                return\n",
    "            if mode is upper and line == 'title':\n",
    "                c.sendall(b'<switching to title case mode>\\r\\n')\n",
    "                mode = title\n",
    "                continue\n",
    "            if mode is title and line == 'upper':\n",
    "                line = c.sendall(b'<switching to upper case mode>\\r\\n')\n",
    "                mode = upper\n",
    "                continue\n",
    "            print(sessions[c].address, '-->', line)\n",
    "            if mode is upper:\n",
    "                c.sendall(b'Upper-cased: %a\\r\\n' % line.upper())\n",
    "            else:\n",
    "                c.sendall(b'Title-cased: %a\\r\\n' % line.title())\n",
    "    finally:\n",
    "        print(sessions[c].address, 'quit')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    reactor('localhost', 9600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on code under 'User's Business Logic'.\n",
    "Inside it, we have a function 'announcement' (that prints some information) that needs to be called every 15 seconds.\n",
    "\n",
    "Also, we're runing a server that assynchronously receives a connection from multiple soruces (users) that receives a sequence of characters and display them either in upper case or title case modes (title mode is when only the first letter of the string is upper case). An user can write 'title' or 'upper'to change their modes.\n",
    "\n",
    "What makes this assyncrhonous?\n",
    "\n",
    "1. The 'async def' at the beggining of the function.\n",
    "2. Everywhere we would have blocked we use a non-blocking version of it. For example, we're using a non-blocking version of readline, putting it in a 'await'.\n",
    "\n",
    "Otherwise, this code looks almost exactly as the single-process version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 1:11:16 on Keynote on Concurrency video, Raymond presents a testing where two different users access the same server. During the testing an user A changes its mode to 'upper'. Another user, B, that haven't changes it's mode continues to receive responses in 'title' (default) mode. Thus, every user has its own individual state, and that was as easy to write as the single process version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
