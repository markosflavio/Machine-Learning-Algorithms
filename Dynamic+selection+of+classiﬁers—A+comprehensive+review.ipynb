{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic selection of classiﬁers—A comprehensive review (Britto Jr)\n",
    "<p class='lead'>\n",
    "Author: Oliveira, Markos F. B. G.<br />\n",
    "Date: 8/25/2017\n",
    "</p>\n",
    "\n",
    "# Description\n",
    "\n",
    "In this notebook the algorithms described in the paper *Dynamic selection of classiﬁers—A comprehensive review* are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Wine data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "raw_data = urllib.request.urlopen(url)\n",
    "dataset = np.loadtxt(raw_data, delimiter=\",\")\n",
    "Xw = dataset[:, 1:]\n",
    "yw = dataset[:, 0]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xw, yw, stratify=yw, train_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual-based measures\n",
    "\n",
    "### (3.1.1) Ranking-based measures\n",
    "\n",
    "#### (1) DSC-Rank method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To be done.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1.2) Accuracy-based measures\n",
    "\n",
    "#### (2) DS-LA OLA-based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def ds_la_ola(pool, X_val, y_val, X_test, k=3):\n",
    "    \"\"\"Returns the predicted values of new instances considering the local best classifier in a pool. If ties occur on\n",
    "    determining the best local classifier, a majority voting is used.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pool : list of sklearn-estimators\n",
    "        List of base-estimators\n",
    "    X_val : np.ndarray\n",
    "        Validation inputs.\n",
    "    y_val : np.ndarray\n",
    "        Validation outputs.\n",
    "    X_test : np.ndarray\n",
    "        Test inputs.\n",
    "    k : int\n",
    "        Neighborhood size.\n",
    "    n_best : int\n",
    "        Number of classifiers to keep.\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : np.ndarray\n",
    "        Predictions on the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    pool = np.array(pool)\n",
    "    if X_test.ndim == 1:\n",
    "        X_test = X_test.reshape(1,-1)\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_val, y_val)\n",
    "    assert(k < X_val.shape[0])\n",
    "    \n",
    "    y_pred = [] #list of predictions for each test instance.\n",
    "    for xi in X_test: #for each test instance.\n",
    "        y_class = []\n",
    "        for cl in pool: #for each classifier\n",
    "            y_class.append(cl.predict([xi])[0])\n",
    "        if y_class.count(y_class[0]) == len(y_class): #check if all classifiers return same label\n",
    "            y_pred.append(y_class[0])\n",
    "        else:\n",
    "            ind = clf.kneighbors(X=[xi], n_neighbors=k, return_distance=False)\n",
    "            X_k = np.take(X_val, ind, axis=0)[0] #data base of k nearest neighbors\n",
    "            y_k = np.take(y_val, ind, axis=0)[0]\n",
    "            model_scores = [] # list of tuples: (model, score)\n",
    "            for cl in pool:#for each classifier\n",
    "                model_scores.append((cl, cl.score(X_k, y_k)))\n",
    "            scs = np.array([x[1] for x in model_scores])\n",
    "            print(scs)\n",
    "            print('opa2')\n",
    "            print(dt1.predict([xi])[0])\n",
    "            print(dt2.predict([xi])[0])\n",
    "            print(dt3.predict([xi])[0])\n",
    "            best_models_ids = np.argwhere(scs == np.amax(scs)) #select the indices of best models (>1 if ties)\n",
    "            best_model_preds = []\n",
    "            for best_cl in pool[np.array(best_models_ids)]:\n",
    "                best_model_preds.append(best_cl[0].predict([xi])[0])\n",
    "            if best_models_ids.shape == (1, 1): #no ties: cover the case where there is no tie between model performances\n",
    "                #considering local validation points (X_l, y_l), i.e., only one model is better and thus, is used to make the\n",
    "                #prediciton on xi.\n",
    "                y_pred.append(pool[best_models_ids[0][0]].predict([xi])[0])\n",
    "                print(pool[best_models_ids[0][0]].predict([xi])[0])\n",
    "            elif best_model_preds.count(best_model_preds[0]) == len(best_model_preds): #no ties: cover the case where there is\n",
    "                #tie bewteen different models performances considering the local validatoin points (X_l, y_l),\n",
    "                #but the predictions of this models in the new instance is the same.\n",
    "                y_pred.append(best_model_preds[0])\n",
    "                print(best_model_preds[0])\n",
    "            else: #ties\n",
    "                print('tie')\n",
    "                (values,counts) = np.unique(np.array(best_model_preds), return_counts=True)\n",
    "                ind=np.argmax(counts)\n",
    "                y_pred.append(values[ind])\n",
    "                print(values[ind])        \n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "\n",
    "- It's important to use validation points, not used in training to validate the local performances os each single classifier. Most classifiers can overfit the training data and could return very-high performances on local subspaces of the feature space. Using a set of points that were not used in training guarantee a unbiased proxy of the generalization error of each individual classifier on the subspace of the new instance.\n",
    "\n",
    "- Ties are very often using this approach, maily for small k's: several classifiers can return the same neighbor validaiton error. In this case, the majority predicted class is returned considering the best classifiers on the k neighbors only. Note that even when using this approach, ties may remain. Suppose Pc1, Pc2 and Pc3 as performances on the k neighbors of classifiers C1, C2 and C3 respectively. In addition, consider the predicted classes as c1, c2 and c1. If (Pc1==Pc2) and (Pc1>Pc3) it's not clear which class we should return: c1 or c2. One approach whould be to use the probabilities predicted by C1 and C2 and choose the predicted class, whose classifier (among the best ones) has more confidence (higher probability).\n",
    "\n",
    "- Majority voting may not be the best approach to decide the prediciton, when ties in the k-neigbors occur. One example that shwoed up in the trials are the following: three classifiers has local performances 0.4, 0.4, 0.4; i.e. a tie occured. The classes predicted were 2, 1, 1. The confidences for each prediciton were [0, 1, 0], [0.5, 0, 0.5] and [0.5, 0, 0.5] respectively. Because majority voting was used, class one was returned. It can be seen that even though the most common class were 1, the individual confidences were very low. Probably, a good approach to avoid this is to predict the class with the higher mean probability value. The integration parameter when ties occur could be selected by the user. Note however, when we perform integration, we loose the intuition of choosing a single classifier to perform some prediciton.\n",
    "\n",
    "- Should we choose the classifier for an instance where the local performances (for all classifiers) are very low (below random guessing)?\n",
    "\n",
    "- Only the best classifier is returned. It whould be nice to generalize this to N-best classifiers.\n",
    "\n",
    "- How to use the distance between points to icnrease selection performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) DS-LA LA-based method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def ds_la_la(pool, X_val, y_val, X_test, k=5):\n",
    "    \"\"\"Returns the predicted values of new instances considering the local best classifier in a pool. If ties occur on\n",
    "    determining the best local classifier, a majority voting is used.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pool : list of sklearn-estimators\n",
    "        List of base-estimators\n",
    "    X_val : np.ndarray\n",
    "        Validation inputs.\n",
    "    y_val : np.ndarray\n",
    "        Validation outputs.\n",
    "    X_test : np.ndarray\n",
    "        Test inputs.\n",
    "    k : int\n",
    "        Neighborhood size.\n",
    "    n_best : int\n",
    "        Number of classifiers to keep.\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : np.ndarray\n",
    "        Predictions on the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    #A separate storage is done for each target class:\n",
    "    knns = [] #stores tuples (class, base)\n",
    "    for tgt in np.unique(y_val):\n",
    "        #print('t:', X_val[[y_val==tgt]])\n",
    "        clf = KNeighborsClassifier(n_neighbors=k).fit(X_val[[y_val==tgt]], y_val[[y_val==tgt]])\n",
    "        #print(clf.kneighbors(X=[[-1,-1,-1]], n_neighbors=1, return_distance=False))\n",
    "        knns.append((tgt, clf))\n",
    "        assert(k <= X_val[[y_val==tgt]].shape[0])\n",
    "    \n",
    "    pool = np.array(pool)\n",
    "    if X_test.ndim == 1:\n",
    "        X_test = X_test.reshape(1,-1)\n",
    "    \n",
    "    y_pred = [] #list of predictions for each test instance.\n",
    "    for xi in X_test: #for each test instance.\n",
    "        y_class = []\n",
    "        for cl in pool: #for each classifier\n",
    "            y_class.append(cl.predict([xi])[0])\n",
    "        if y_class.count(y_class[0]) == len(y_class): #check if all classifiers return same label\n",
    "            y_pred.append(y_class[0])\n",
    "        else:\n",
    "            model_scores = [] # list of scores (sorted as classifiers are sorted in 'pool')\n",
    "            for pred, cl in zip(y_class, pool): #for each predicted class of classifiers\n",
    "                print('pred:', pred)\n",
    "                knn = [tpl[1] for tpl in knns if tpl[0] == pred]\n",
    "                ind = knn[0].kneighbors(X=[xi], n_neighbors=k, return_distance=False) #nearest neighbors of xi that has the\n",
    "                #same labels returned by classifier cl.\n",
    "                X_k = np.take(X_val[[y_val==pred]], ind, axis=0)[0] #data base of k nearest neighbors\n",
    "                y_k = np.take(y_val[[y_val==pred]], ind, axis=0)[0]\n",
    "                #print(X_k)\n",
    "                model_scores.append((cl, cl.score(X_k, y_k)))\n",
    "            scs = np.array([x[1] for x in model_scores])\n",
    "            print('scores:',scs)\n",
    "            print('opa2')\n",
    "            print(dt1.predict([xi])[0])\n",
    "            print(dt2.predict([xi])[0])\n",
    "            print(dt3.predict([xi])[0])\n",
    "            best_models_ids = np.argwhere(scs == np.amax(scs)) #select the indices of best models (>1 if ties)\n",
    "            best_model_preds = []\n",
    "            for best_cl in pool[np.array(best_models_ids)]:\n",
    "                best_model_preds.append(best_cl[0].predict([xi])[0])\n",
    "            if best_models_ids.shape == (1, 1): #no ties: cover the case where there is no tie between model performances\n",
    "                #considering local validation points (X_l, y_l), i.e., only one model is better and thus, is used to make the\n",
    "                #prediciton on xi.\n",
    "                y_pred.append(pool[best_models_ids[0][0]].predict([xi])[0])\n",
    "                print(pool[best_models_ids[0][0]].predict([xi])[0])\n",
    "            elif best_model_preds.count(best_model_preds[0]) == len(best_model_preds): #no ties: cover the case where there is\n",
    "                #tie bewteen different models performances considering the local validatoin points (X_l, y_l),\n",
    "                #but the predictions of this models in the new instance is the same.\n",
    "                y_pred.append(best_model_preds[0])\n",
    "                print(best_model_preds[0])\n",
    "            else: #ties\n",
    "                print('tie')\n",
    "                (values,counts) = np.unique(np.array(best_model_preds), return_counts=True)\n",
    "                ind=np.argmax(counts)\n",
    "                y_pred.append(values[ind])\n",
    "                print(values[ind])        \n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "\n",
    "- The same remarks pointed about OLA method makes sense here, because the high similarity between both merging methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1.3) Probabilistic-based measures\n",
    "\n",
    "#### (4) A Priori/A Posteriori method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def a_priori_posteriori(pool, X_val, y_val, X_test, k=5, method='priori', threshold=0.1, low_conf='rnd'):\n",
    "    \"\"\"Returns the predicted values of new instances considering the local best classifier in a pool. If ties occur on\n",
    "    determining the best local classifier, a majority voting is used.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pool : list of sklearn-estimators\n",
    "        List of base-estimators\n",
    "    X_val : np.ndarray\n",
    "        Validation inputs.\n",
    "    y_val : np.ndarray\n",
    "        Validation outputs.\n",
    "    X_test : np.ndarray\n",
    "        Test inputs.\n",
    "    k : int\n",
    "        Neighborhood size.\n",
    "    n_best : int\n",
    "        Number of classifiers to keep.\n",
    "    approach : str\n",
    "        Method used to compute local accuracies. Valid values: 'priori', 'posteriori'.\n",
    "    threshold: float\n",
    "        Threshold used to compute the level of confidence of the selected classifier with maximum p_correct.\n",
    "    low_conf: float\n",
    "        Method used to predict the value of the new instance if the maximum p_correct is not above than the\n",
    "        threshold of others. If 'rnd' is selected a random classifier with p_correct value higher than max p_correct - threshold\n",
    "        is selected. If 'ola' the approch based on local accuracy proposed by Woods is used. Valid values: 'rnd', 'ola'.\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : np.ndarray\n",
    "        Predictions on the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert((method == 'priori') | (method == 'posteriori'))\n",
    "    assert((low_conf == 'rnd') | (low_conf == 'ola'))\n",
    "    \n",
    "    #Full storage.\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_val, y_val)\n",
    "    assert(k < X_val.shape[0])\n",
    "    \n",
    "    pool = np.array(pool)\n",
    "    if X_test.ndim == 1:\n",
    "        X_test = X_test.reshape(1,-1)\n",
    "    \n",
    "    y_pred = [] #list of predictions for each test instance.\n",
    "    for instance, xi in enumerate(X_test): #for each test instance.\n",
    "        #print('*********Instance:', instance)\n",
    "        y_class = []\n",
    "        for cl in pool: #for each classifier\n",
    "            y_class.append(cl.predict([xi])[0])\n",
    "        if y_class.count(y_class[0]) == len(y_class): #check if all classifiers return same label\n",
    "            y_pred.append(y_class[0])\n",
    "        else:\n",
    "            \n",
    "            ind = knn.kneighbors(X=[xi], n_neighbors=k, return_distance=False)\n",
    "            X_k = np.take(X_val, ind, axis=0)[0] #data base of k nearest neighbors\n",
    "            y_k = np.take(y_val, ind, axis=0)[0]\n",
    "            \n",
    "            p_corrects = [] # list of tuples\": stores p_corrects for each classifier index in the pool.\n",
    "            for i, (pred, cl) in enumerate(zip(y_class, pool)): #for class/predicted class\n",
    "                \n",
    "                #See steps at section 3.4 of Giacinto's paper 'Methods for Dynamic Classifier Selection'\n",
    "                #STEP 1. Compute p_correct\n",
    "                if method == 'priori':\n",
    "                    nom = 0\n",
    "                    deltas = []\n",
    "                    for row_x, row_y in zip(X_k, y_k):\n",
    "                        delta_i = 1/(np.linalg.norm([xi]-row_x))\n",
    "                        deltas.append(delta_i)\n",
    "                        itemindex = np.where(cl.classes_==row_y)[0][0]\n",
    "                        P_hat_i = cl.predict_proba([row_x])[0][itemindex] #probability of current classifier (cl)\n",
    "                        #predict the correct class of neighbor row_x, row_y.\n",
    "                        nom = nom + P_hat_i*delta_i\n",
    "                    p_correct = nom/sum(deltas)\n",
    "                    \n",
    "                else: #posteriori\n",
    "                    nom = 0.0\n",
    "                    den = 0.0\n",
    "                    for row_x, row_y in zip(X_k, y_k):\n",
    "                        delta_i = 1/(np.linalg.norm([xi]-row_x))\n",
    "                        #print('Delta_i', delta_i)\n",
    "                        itemindex = np.where(cl.classes_== pred)[0][0]\n",
    "                        #print('Predict proba:', cl.predict_proba([row_x])[0][itemindex])\n",
    "                        den = den + cl.predict_proba([row_x])[0][itemindex]*delta_i\n",
    "                        if row_y == pred:\n",
    "                            nom = nom + cl.predict_proba([row_x])[0][itemindex]*delta_i\n",
    "                    p_correct = nom/den\n",
    "                    #print('Nom:', nom)\n",
    "                    #print('Den:', den)\n",
    "                    #print('p_correct:', p_correct)\n",
    "                    \n",
    "                #STEP 2. Reject classifiers if p_correct<0.5\n",
    "                if (p_correct < 0.5) | (p_correct != p_correct): #reject classifier. The second condition is for nan case.\n",
    "                    continue #goes to the next classifier\n",
    "                else:\n",
    "                    p_corrects.append((i, p_correct))\n",
    "            \n",
    "            if not p_corrects: #if the list is empty use the best classifier in all validation set. Handle the cases where\n",
    "                #the predict_proba for pred class (in the posteriori case) is 0 (which causes a p_correct == nan).\n",
    "                print('Any p_correct > 0.5 was found!')\n",
    "                scores = []\n",
    "                for cl in pool:\n",
    "                    scores.append(cl.score(X_val, y_val))\n",
    "                y_out = pool[np.argmax(np.array(scores))].predict([xi])[0]\n",
    "                y_pred.append(y_out)\n",
    "                #print('Pred all validation:', y_out)\n",
    "                continue #goes to the next instance    \n",
    "            \n",
    "            #STEP 3. Identify Cj with max p_correct\n",
    "            p_corrects = sorted(p_corrects, key=lambda x: x[1]) #sort in ascending order\n",
    "            p_max = p_corrects[-1][1] #if there is more than one pmax?\n",
    "            c_max = pool[p_corrects[-1][0]]\n",
    "            #print('p_corrects:', p_corrects)\n",
    "            #print('p_max:', p_max)\n",
    "            \n",
    "            #STEP 4. Compute differences beween p_max and P_correct in p_corrects\n",
    "            diff = [] #list of tuples (cl_index, difference)\n",
    "            if method == 'priori':\n",
    "                for tpl in p_corrects[:-1]: #removing only the max value.\n",
    "                    dif = p_max - tpl[1]\n",
    "                    diff.append((tpl[0], dif))\n",
    "            else:#If 'a posteriori' method is selected, then the difference dj is evaluated only for the classifiers\n",
    "            #that take a decision different from the one taken by c_max.\n",
    "                c_max_pred = c_max.predict([xi])[0]\n",
    "                #print('c_max_pred:', c_max_pred)\n",
    "                for tpl in p_corrects[:-1]:\n",
    "                    tpl_pred = pool[tpl[0]].predict([xi])[0]\n",
    "                    #print('tpl_pred:', tpl_pred)\n",
    "                    if not tpl_pred == c_max_pred:\n",
    "                        dif = p_max - tpl[1]\n",
    "                        diff.append((tpl[0], dif))\n",
    "            #print('Diff:', diff)\n",
    "            if not diff: #if diff is empty\n",
    "                y_pred.append(c_max.predict([xi])[0])\n",
    "                #print('Pred empty:', c_max.predict([xi])[0])\n",
    "                continue\n",
    "            #STEP. 5 Checks the 'confidence' of the selected c_max classifier\n",
    "            if all(dif[1] > threshold for dif in diff):\n",
    "                y_pred.append(c_max.predict([xi])[0]) #high confidence, c_max can be selected safely\n",
    "                #print('Pred threshold:', c_max.predict([xi])[0])\n",
    "            else: #low confidence: random selection of good classifiers\n",
    "                \n",
    "                ids = [tpl[0] for tpl in diff if tpl[1] < threshold] #indices of classifiers in p_corrects that have high values\n",
    "                #(low difference), it does not include the c_max index.\n",
    "                ids = np.array(ids)\n",
    "                #print('Ids before append:', ids)\n",
    "                ids = np.append(ids, -1) #adding the c_max index.\n",
    "                #print('Ids after append:', ids)\n",
    "                if low_conf == 'rnd':\n",
    "                    random_id = np.random.choice(ids)\n",
    "                    random_cl = pool[p_corrects[int(random_id)][0]]\n",
    "                    y_pred.append(random_cl.predict([xi])[0])\n",
    "                    #print('Pred random:', random_cl.predict([xi])[0])\n",
    "                else:\n",
    "                    #list of 'good'classifiers:\n",
    "                    good_cl = []\n",
    "                    for idd in ids:\n",
    "                        good_cl.append(pool[p_corrects[idd][0]]) #at least two classifiers will be on this list.\n",
    "                    y_out = ds_la_ola(good_cl, X_val, y_val, xi, k=k)\n",
    "                    y_pred.append(y_out[0])\n",
    "                    \n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "\n",
    "- When randomly selecting the classifier with reasonable p_correct value, it would be possible to instead of using a uniform distribution of the classifiers, use a distribution based on the value itself: classifiers with higher p_correct values have more chance to be selected.\n",
    "\n",
    "- It seems that's not a good idea to use the random strategy to choose a classifier, particularly in the 'a posteriori' case. It's not clear why should we use only the classifiers that take a decision different from the one in c_max. This may cause a lot of classifiers that have different predicted class than p_max; and in the random method, the c_max has little chance to win (even in the ola ensemble method). In addition, in the experiments was common to see only two remaining classifiers with different predicted labels. We should make a lot of effort to classify this instance as correct, becaue probably, such instance is a hard case, and classifing it correclty is what makes a successfull classification strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train = np.array([[0,0,0],[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5],[6,6,6],[7,7,7],[8,8,8],[9,9,9]])\n",
    "#y_train = np.array([0,1,0,1,0,1,0,1,0,1])\n",
    "#X_test = np.array([[2.1,2.1,2.1],[-1,-1,-1]])\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#dt1 = DecisionTreeClassifier(max_depth=3)\n",
    "#dt1.fit(X_train, y_train)\n",
    "#dt2 = DecisionTreeClassifier(max_depth=2, random_state=9)\n",
    "#dt2.fit(X_train, y_train)\n",
    "#dt3 = DecisionTreeClassifier(max_depth=1, random_state=9)\n",
    "#dt3.fit(X_train, y_train)\n",
    "y = a_priori_posteriori([dt1, dt2, dt3], X_train, y_train, X_test, k=5, method = 'priori', low_conf='rnd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1.4) Behavior-based measures\n",
    "\n",
    "#### (5) DS-MCB method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def ds_mcb(pool, X_val, y_val, X_test, k=5, threshold=0.1, ola_threshold=0.1):\n",
    "    \"\"\"Returns the predicted values of new instances considering the local best classifier in a pool. If ties occur on\n",
    "    determining the best local classifier, a majority voting is used.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pool : list of sklearn-estimators\n",
    "        List of base-estimators\n",
    "    X_val : np.ndarray\n",
    "        Validation inputs.\n",
    "    y_val : np.ndarray\n",
    "        Validation outputs.\n",
    "    X_test : np.ndarray\n",
    "        Test inputs.\n",
    "    k : int\n",
    "        Neighborhood size.\n",
    "    n_best : int\n",
    "        Number of classifiers to keep.\n",
    "    approach : str\n",
    "        Method used to compute local accuracies. Valid values: 'priori', 'posteriori'.\n",
    "    threshold: float\n",
    "        Threshold used to define if a neighbor is similar to the new instance according to their predicitons.\n",
    "    ola_threshold: float\n",
    "        Threshold used to define if the classifier with maximum local accuracy is bigger enough to be used safely.\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : np.ndarray\n",
    "        Predictions on the test set.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Full storage.\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_val, y_val)\n",
    "    assert(k < X_val.shape[0])\n",
    "    \n",
    "    pool = np.array(pool)\n",
    "    if X_test.ndim == 1:\n",
    "        X_test = X_test.reshape(1,-1)\n",
    "    \n",
    "    y_pred = [] #list of predictions for each test instance.\n",
    "    for instance, xi in enumerate(X_test): #for each test instance.\n",
    "        #print('*********Instance:', instance)\n",
    "        y_class = []\n",
    "        for cl in pool: #for each classifier\n",
    "            y_class.append(cl.predict([xi])[0])\n",
    "        if y_class.count(y_class[0]) == len(y_class): #check if all classifiers return same label\n",
    "            y_pred.append(y_class[0])\n",
    "        else:\n",
    "            mcb_t = y_class\n",
    "            #Find nearest neighbors\n",
    "            ind = knn.kneighbors(X=[xi], n_neighbors=k, return_distance=False)\n",
    "            X_k = np.take(X_val, ind, axis=0)[0] #data base of k nearest neighbors\n",
    "            y_k = np.take(y_val, ind, axis=0)[0]\n",
    "            \n",
    "            mcbs = [] #list of tuples. Each tuple has row_x, row_y and list of similarity. Number of internal tuples/lists =\n",
    "            #number of neighbors; number of entries in each list = number of classifiers in the pool.\n",
    "            for row_x, row_y in zip(X_k, y_k):\n",
    "                mcb_n = []\n",
    "                for cl in pool:\n",
    "                    pd = cl.predict([row_x])[0]\n",
    "                    mcb_n.append(pd)\n",
    "                mcbs.append((row_x, row_y, mcb_n))\n",
    "            \n",
    "            sim_neighbors_x = []\n",
    "            sim_neighbors_y = []\n",
    "            while not sim_neighbors_x: #avoids 'sim_neighbors' being empty.\n",
    "                for mcb_j in mcbs:\n",
    "                    sim = np.mean(np.array([m==n for m, n in zip(mcb_j[2], mcb_t)]))\n",
    "                    if sim > threshold:\n",
    "                        sim_neighbors_x.append(mcb_j[0])\n",
    "                        sim_neighbors_y.append(mcb_j[1])\n",
    "                if not sim_neighbors_x: \n",
    "                    threshold = threshold + 0.1;\n",
    "            \n",
    "            scores = [] #list of tuples (cl_idx, score)\n",
    "            for i, cl in enumerate(pool):\n",
    "                Xv = np.array(sim_neighbors_x)\n",
    "                yv = np.array(sim_neighbors_y)\n",
    "                scores.append((i, cl.score(Xv, yv))) #local accuracy of neighbor with similar patterns than new instance\n",
    "            \n",
    "            scores = sorted(scores, key=lambda x: x[1])\n",
    "            \n",
    "            if all(sc[1] > scores[-1][1] - ola_threshold for sc in scores[:-1]): #scores[-1][1] is the maximum score\n",
    "                ids = [sc[0] for sc in scores if sc[1] > scores[-1][1] - ola_threshold] #ids of classifiers with 'high' score.\n",
    "                preds = []\n",
    "                for idd in ids:\n",
    "                    preds.append(pool[idd].predict([xi])[0])\n",
    "                y_pred.append(np.bincount(preds).argmax()) #returns the most frequent prediction\n",
    "            else: #high confidence in the max\n",
    "                y_pred.append(pool[scores[-1][0]].predict([xi])[0])\n",
    "                    \n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "    \n",
    "- The original paper states \"If the classifier outputs can be regarded as estimates of the class posterior probabilities, these\n",
    "probabilities can be taken into account in order to improve the estimation of CLA.\" A majority scheme was used in this algorithm, but it could be used the information of 'predict_proba' to enhance accuracy.\n",
    "\n",
    "- A step not presented in the original algorithm was implemented. It iteratively increases the similarity threshold by 10% until neighbors with similar MCB are found. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3.1.5) Oracle-based measures\n",
    "\n",
    "#### (6) KNE method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To be done.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
