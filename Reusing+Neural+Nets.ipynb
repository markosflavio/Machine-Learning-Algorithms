{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusing Neural Networks with Tensorflow\n",
    "<p class='lead'>\n",
    "Author: Oliveira, Markos F. B. G.<br />\n",
    "Date: 8/22/2017\n",
    "</p>\n",
    "\n",
    "# Description\n",
    "\n",
    "This document is based on Chapter 11 content of Geron's book *Hands-On Machine Learning with Scikit-Learn & TensorFlow*. It covers only the basics of the related topics and it's not meant to be a tutorial. Please, refer to the referenced book and scikit-learn/tensorflow online documentation for specific information.\n",
    "\n",
    "In this Notebook an example of reusing neural networks is presented. It's a simple case that demostrates how reusing the weights of a pre-trained deep neural network (DNN) can help modeling similar concepts in another (but related) supervised problem. This can provide a better generalization capabillity of the new model even in cases of lack of data. The information that would be on the data, it's encoded now in the weights of the old model, reused by the new model. Because most of ML problems deal with hierarchical structures (such as images, where high level structures such as faces can be constructed using low level structures such as small lines, and corners), it makes sense to reuse learned low level structures/feature detectors in further related problems (i.g. hair recognition system can be build from a DNN that learned to recognize faces).\n",
    "\n",
    "A DNN makes it possible to encode concepts with different granularities due to its hierarchical structure. A general approach is to fetch the weights of some layers of the old DNN, frozen the lower layers and train only the high level layers. Note that even though we have a high complexity mapping (deep net), just some parameters are trainable. Thus, less data is necessary to construct such mapping. As the similarity of the related problems increases, more layers can be reused.\n",
    "\n",
    "## Problem Description\n",
    "\n",
    "In particular, in this Notebook it's shown how to reuse part of a deep net that learned hot to recognize the difference between '5's and '3's hadwritten digits, in another recognition problem: recognize the difference between '8's and '9's digits. For the second problem, less data is presented, to demonstrade the usefulness of such methodology in cases where the available data is not big enough to find a good representation. The digits used is from MNIST database.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- The presented example is simple, the database of the two training procedure is the same: MNIST. Thus, the number of features is the same in both examples. It's not clear how to reuse a network that was trained in a previous problem with different number of features.\n",
    "\n",
    "- The training procedure is not optimized. Due to the fact that this is a small learning problem, any optimized techniques related to weight initialization, normalization, optimization, activation functions, regularization, etc. were used. Hence, plain stochastic gradient descent is used in this work.\n",
    "\n",
    "- To gain a speed boost it's possible to cache the output of a frozen layers and run SGB using this as inputs. This will give you a huge speed boost as you will only need to go through the frozen layers once per training instance, instead of once per epoch. However, this was not implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries/modules/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and pre-processing the Datasets\n",
    "\n",
    "It's necessary to create two datasets, one for each recognition problem. No validation set will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#Downloading and extracting the data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "#Fetching the data (no validation set will be used in this project)\n",
    "X_train_35 = mnist.train.images[(mnist.train.labels.astype(\"int\") == 3) + (mnist.train.labels.astype(\"int\") == 5)]\n",
    "y_train_35 = mnist.train.labels.astype(\"int\")[(mnist.train.labels.astype(\"int\") == 3) + (mnist.train.labels.astype(\"int\") == 5)]\n",
    "y_train_35 = y_train_35 == 5 #transform to 1s and 0s vector\n",
    "X_test_35 = mnist.test.images[(mnist.test.labels.astype(\"int\") == 3) + (mnist.test.labels.astype(\"int\") == 5)]\n",
    "y_test_35 = mnist.test.labels.astype(\"int\")[(mnist.test.labels.astype(\"int\") == 3) + (mnist.test.labels.astype(\"int\") == 5)]\n",
    "y_test_35 = y_test_35 == 5 #transform to 1s and 0s vector\n",
    "X_train_89 = mnist.train.images[(mnist.train.labels.astype(\"int\") == 8) + (mnist.train.labels.astype(\"int\") == 9)]\n",
    "y_train_89 = mnist.train.labels.astype(\"int\")[(mnist.train.labels.astype(\"int\") == 8) + (mnist.train.labels.astype(\"int\") == 9)]\n",
    "y_train_89 = y_train_89 == 9 #transform to 1s and 0s vector\n",
    "X_test_89 = mnist.test.images[(mnist.test.labels.astype(\"int\") == 8) + (mnist.test.labels.astype(\"int\") == 9)]\n",
    "y_test_89 =  mnist.test.labels.astype(\"int\")[(mnist.test.labels.astype(\"int\") == 8) + (mnist.test.labels.astype(\"int\") == 9)]\n",
    "y_test_89 = y_test_89 == 9 #transform to 1s and 0s vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the First DNN (3's vs 5's)\n",
    "\n",
    "To train the DNNs using tensorflow, it will be used SGD algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape of training data:  (10625, 784)\n",
      "Outputs shape of training data:  (10625,)\n",
      "# inputs:  784\n",
      "# outputs:  2\n",
      "# instances:  10625\n"
     ]
    }
   ],
   "source": [
    "#Storing some important values\n",
    "X_train = X_train_35\n",
    "X_test = X_test_35\n",
    "y_train = y_train_35\n",
    "y_test = y_test_35\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = (np.unique(y_train)).size\n",
    "m = X_train.shape[0]\n",
    "print('Inputs shape of training data: ', X_train.shape)\n",
    "print('Outputs shape of training data: ', y_train.shape)\n",
    "print('# inputs: ', n_inputs) # 28*28 = MNIST\n",
    "print('# outputs: ', n_outputs)\n",
    "print('# instances: ', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In stochastic gradient descent it's necessary to provide stochastic small batches of examples so that the algorithm updates more often and eventually converges faster. The function returns a mini-batch when its called using two approaches.\n",
    "\n",
    "- In the 1st approach random indices from dataset are selected to be inside the mini-batch using a randomized seed, which is the number of the training iteration (epoch*n_batches + batch_index). In this case, one epoch does not pass through all the different examples.\n",
    "- In the 2nd approach a random permutation of the examples is passed and a slice of this permutation is used as mini-batch. In this case all the examples are used in one epoch, thus it's a better approach. A random permutation of the examples is necessary to be computed once per epoch oustide the iteration over the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size, idxs, Xs, ys, approach=2):\n",
    "    \"\"\"Returns the mini-batch (X, y) for a training step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : int\n",
    "        Number of epoch.\n",
    "    batch_index : int\n",
    "        Number of batch inside the epoch.\n",
    "    batch_size : int\n",
    "        Batch size.\n",
    "    idxs : list of int\n",
    "        Permutation of training set indices.\n",
    "    approach: int, optional\n",
    "        Approach used for chosing the mini-batches. \n",
    "    Returns\n",
    "    -------\n",
    "    X_batch : np.ndarray\n",
    "        Mini-batch of inputs.\n",
    "    y_batch : np.ndarray\n",
    "        Mini-batch of outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    if approach == 1:\n",
    "        # 1st approach: in this approach random indices from dataset are selected to be inside the mini-batch using a\n",
    "        #randomized seed, which is the number of the training iteration. In this case, one epoch probably does\n",
    "        #not have all the different examples.\n",
    "        rnd.seed(epoch * n_batches + batch_index)\n",
    "        indices = rnd.randint(m, size=batch_size)\n",
    "    elif approach == 2:\n",
    "        # 2nd approach: in this approach a random permutation of the examples is passed and a slice of this permutation\n",
    "        #is used as mini-batch.\n",
    "        indices = idxs[batch_index * batch_size: (batch_index + 1) * batch_size]\n",
    "    \n",
    "    X_batch = Xs[indices]\n",
    "    y_batch = ys[indices]\n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to have a function that manually creates the archictecure of a neural network. In this way, it's possible to create structures that are not fully connected. Below, it's implemented a function that creates a standard feedforward network. The function creates each layer at a time when it's called, and not the fully structure. The weights are initialized using common method found in literature to speed up convergence and avoid saturation in early stages of learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    \"\"\"Manually creates the layers of the neural network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Input values of the layer (m_batch, n_inputs).\n",
    "    n_neurons : int\n",
    "        Number of neurons in the layer.\n",
    "    name : string\n",
    "        Scope name of the layer.\n",
    "    activation : string\n",
    "        Type of activation function.\n",
    "    Returns\n",
    "    -------\n",
    "    z : np.ndarray\n",
    "        The output of the layer.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs) # good strategy to initialize the NN's weights.\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev, seed = 0) # using truncated distribution we avoid\n",
    "        #values whose magnitude is more than 2 standard deviations (95%) from the mean, which is zero (these values are\n",
    "        #dropped and re-picked). It helps with convergence speed.\n",
    "        W = tf.Variable(init, name=\"weights\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"biases\")\n",
    "        z = tf.matmul(X, W) + b #If m_match > 1 tf.matmul(X, W) is a matrix; b i summed to all of its columns.\n",
    "        \n",
    "        if activation==\"relu\":\n",
    "            return tf.nn.relu(z)\n",
    "        elif activation==\"sigmoid\":\n",
    "            return tf.nn.sigmoid\n",
    "        elif activation==\"tanh\":\n",
    "            return tf.nn.tanh\n",
    "        else:\n",
    "            return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, some of the training paramters can be set. The archictecture of the DNN though, is set in the construction phase of the TF graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SGD parameters:\n",
    "n_epochs = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "#Monitoring:\n",
    "prt = True # enable printing training statistics.\n",
    "print_step = 10 # number of epochs to periodically print training statistics.\n",
    "\n",
    "#Neural networks layers:\n",
    "manual_layers = True #manually creates the layers using 'neural_layer' function.\n",
    "tf_batch = fetch_batch # use implemented TF function to load mini-batches.\n",
    "#for MNIST: tf_batch = mnist.train.next_batch(batch_size)\n",
    "n_hidden1 = 200\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 100\n",
    "#Obs.: the archictecture of the layer must be constructed inside the 'CONSTRUCTION PHASE' of the nn graph.\n",
    "\n",
    "#Save/restore:\n",
    "restore = False # restore old model\n",
    "save_ckpt = False # save chackpoints\n",
    "saver_step = 10 # number of epochs to periodically save model's checkpoint.\n",
    "path_restore = '/tmp/finals/my_model_final.ckpt'\n",
    "\n",
    "#Tensorboard:\n",
    "tb = False # log training statistics for tensorboard.\n",
    "root_logdir = \"tf_logs\"\n",
    "tensorboard_step = 10 # number of epochs to periodically log statistics in tensorboard log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we effectively construct and execute the TF graph of the first DNN. The archictecture chosen have three hidden layers with 200, 100 and 100 ReLU neurons. This implementation does not includes early-stopping, hence, with 100 epochs the net highly overfitts the training data, getting a perfect score after the 30/40 training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training accuracy: 0.955671\n",
      "Epoch 10 Training accuracy: 0.994071\n",
      "Epoch 20 Training accuracy: 0.998965\n",
      "Epoch 30 Training accuracy: 0.999718\n",
      "Epoch 40 Training accuracy: 1.0\n",
      "Epoch 50 Training accuracy: 1.0\n",
      "Epoch 60 Training accuracy: 1.0\n",
      "Epoch 70 Training accuracy: 1.0\n",
      "Epoch 80 Training accuracy: 1.0\n",
      "Epoch 90 Training accuracy: 1.0\n",
      "Epoch 100 Training accuracy: 1.0\n",
      "Best model saved as: ./tmp/best_model-20170821195934.ckpt\n",
      "Training time: 42.365 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #restoring the default graph.\n",
    "tf.logging.set_verbosity(tf.logging.WARN) #supress TF logging messages when saving ckpt files.\n",
    "start_time = time.time()\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now) #relative path of tensorboard logs for a particular run (current time\n",
    "#is used so that each folder has different running stats, comparision between them can be made inside tensorboard).\n",
    "\n",
    "# TF CONSTRUCTION PHASE\n",
    "\n",
    "# 1- Creating variables, placeholders and constants.\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "# 2- Creating the operations.\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    if manual_layers:\n",
    "        hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "        hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "        hidden3 = neuron_layer(hidden2, n_hidden3, \"hidden3\", activation=\"relu\")\n",
    "        logits = neuron_layer(hidden3, n_outputs, \"output\") # the final layer returns the logits only.\n",
    "        softmax = tf.nn.softmax(logits, name=\"softmax\")\n",
    "    else:\n",
    "        hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "        hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\", activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")\n",
    "        softmax = tf.nn.softmax(logits, name=\"softmax\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) #this computes the cross entropy based \n",
    "    #directly on the logits of the output layer: it expects integer labels (from 0 to n_outputs-1). This returns the cross-entropy\n",
    "    #scalar value for each instance.\n",
    "    #Use 'softmax_cross_entropy_with_logits()' if the labels are in the form of one-hot vectors.\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\") #computes the mean over the mini_batch.\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) #returns boolean == True if the output y is in the first k highest probabilities.  \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) #computes the accuracy.\n",
    "\n",
    "# 3- Node that initialize the variables.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 4- Creating the saver.\n",
    "saver_save = tf.train.Saver(max_to_keep=None) #save all variable values.\n",
    "if restore:\n",
    "    saver_restore = tf.train.Saver() #restore all variable values.\n",
    "    #saver_restore = tf.train.Saver({\"weights\": theta}) #restore only the old theta variable under the name of 'weights'.\n",
    "\n",
    "# 5- Tensorboard definitions:\n",
    "if tb:\n",
    "    accuracy_summary = tf.summary.scalar('ACC', accuracy) # Creates a node in the graph that will evaluate the ACC value\n",
    "    #and write it to a TensorBoard-compatible binary log string called a summary.\n",
    "    summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) # Creates a FileWriter that you will\n",
    "    #use to write summaries to logfiles in the log directory.\n",
    "\n",
    "# TF EXECUTION PHASE:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # To restore a model the construction phase must be identical than the one used to save it. \n",
    "    if restore:\n",
    "        saver_restore.restore(sess, path_restore)\n",
    "    else:\n",
    "        sess.run(init) # Initializing the variables.\n",
    "    \n",
    "    for epoch in range(n_epochs+1): # for each epoch..\n",
    "        \n",
    "        idXs = np.random.permutation(range(m))\n",
    "        \n",
    "        for batch_index in range(n_batches-1): # for each mini-batch..\n",
    "            X_batch, y_batch = tf_batch(epoch, batch_index, batch_size, idXs, X_train, y_train)\n",
    "            #Training step:\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        if (prt) and (epoch % print_step == 0):\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "            print(\"Epoch\", epoch, \"Training accuracy:\", acc_train)\n",
    "        if (tb) and (epoch % tensorboard_step == 0):\n",
    "            summary_str = accuracy_summary.eval(feed_dict={X: X_train, y: y_train}) # This will output a summary that\n",
    "            #you can then write to the events file using the file_writer. Here is the updated code:\n",
    "            step = epoch * n_batches + batch_index\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "        if (save_ckpt) and (epoch % saver_step == 0):\n",
    "            save_path = saver_save.save(sess, \"./tmp/my_model-{}.ckpt\".format(epoch))\n",
    "\n",
    "    #Saving the model\n",
    "    save_path = saver_save.save(sess, \"./tmp/best_model-{}.ckpt\".format(now)) \n",
    "    \n",
    "# Flushing and closing FileWriter        \n",
    "if tb:                            \n",
    "    summary_writer.flush()\n",
    "    summary_writer.close() \n",
    "\n",
    "best_model_path = \"./tmp/best_model-{}.ckpt\".format(now)\n",
    "print(\"Best model saved as:\", best_model_path)\n",
    "print(\"Training time: %.6s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading and Testing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in the test set:  0.992639327024\n"
     ]
    }
   ],
   "source": [
    "saver_restore = tf.train.Saver() \n",
    "with tf.Session() as sess:\n",
    "    saver_restore.restore(sess, best_model_path)\n",
    "    Z = logits.eval(feed_dict={X: X_test})\n",
    "    y_prob = softmax.eval(feed_dict={X: X_test})\n",
    "    y_pred = np.argmax(Z, axis=1) #or np.argmax(y_prob, axis=1)\n",
    "    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy in the test set: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the second DNN (8's vs 9's) reusing pre-trained weights\n",
    "\n",
    "In the first fully-connected DNN we had (784\\*200)+(200\\*100)+(100\\*100)+(100\\*2)= 187k parameters. Note that freezing the first layer, we have only ~30k learnable parameters, a great reduction. Let's see our performance in the test set when reusing the weights between the input layer and the first hidden layer with 200 neurons and the weights between this layer and the second hidden layer with 100 layers. However, only the first batch of weights will be frozen, the second batch will be used as initial values only.\n",
    "\n",
    "The main differences in the code resort inside the step-4 in the construction phase 'Creating the Saver' and in the optimizer definition under the 'train' scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape of training data:  (10843, 784)\n",
      "Outputs shape of training data:  (10843,)\n",
      "# inputs:  784\n",
      "# outputs:  2\n",
      "# instances:  10843\n"
     ]
    }
   ],
   "source": [
    "#Storing some important values\n",
    "X_train = X_train_89\n",
    "X_test = X_test_89\n",
    "y_train = y_train_89\n",
    "y_test = y_test_89\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = (np.unique(y_train)).size\n",
    "m = X_train.shape[0]\n",
    "print('Inputs shape of training data: ', X_train.shape)\n",
    "print('Outputs shape of training data: ', y_train.shape)\n",
    "print('# inputs: ', n_inputs) # 28*28 = MNIST\n",
    "print('# outputs: ', n_outputs)\n",
    "print('# instances: ', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Training accuracy: 0.970765\n",
      "Epoch 10 Training accuracy: 0.994743\n",
      "Epoch 20 Training accuracy: 0.99834\n",
      "Epoch 30 Training accuracy: 0.999539\n",
      "Epoch 40 Training accuracy: 0.999816\n",
      "Epoch 50 Training accuracy: 0.999908\n",
      "Epoch 60 Training accuracy: 0.999908\n",
      "Epoch 70 Training accuracy: 1.0\n",
      "Epoch 80 Training accuracy: 1.0\n",
      "Epoch 90 Training accuracy: 1.0\n",
      "Epoch 100 Training accuracy: 1.0\n",
      "Training time: 57.358 seconds\n",
      "Accuracy in the test set:  0.989914271306\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #restoring the default graph.\n",
    "tf.logging.set_verbosity(tf.logging.WARN) #supress TF logging messages when saving ckpt files.\n",
    "start_time = time.time()\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now) #relative path of tensorboard logs for a particular run (current time\n",
    "#is used so that each folder has different running stats, comparision between them can be made inside tensorboard).\n",
    "\n",
    "# TF CONSTRUCTION PHASE\n",
    "\n",
    "# 1- Creating variables, placeholders and constants.\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "# 2- Creating the operations.\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    if manual_layers:\n",
    "        hidden1 = neuron_layer(X, n_hidden1, \"hidden1\", activation=\"relu\")\n",
    "        hidden2 = neuron_layer(hidden1, n_hidden2, \"hidden2\", activation=\"relu\")\n",
    "        hidden3 = neuron_layer(hidden2, n_hidden3, \"hidden3\", activation=\"relu\")\n",
    "        logits = neuron_layer(hidden3, n_outputs, \"outputs\") # the final layer returns the logits only.\n",
    "        softmax = tf.nn.softmax(logits, name=\"softmax\")\n",
    "    else:\n",
    "        hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "        hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "        hidden3 = tf.layers.dense(hidden2, n_hidden3, name=\"hidden3\", activation=tf.nn.relu)\n",
    "        logits = tf.layers.dense(hidden3, n_outputs, name=\"outputs\")\n",
    "        softmax = tf.nn.softmax(logits, name=\"softmax\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) #this computes the cross entropy based \n",
    "    #directly on the logits of the output layer: it expects integer labels (from 0 to n_outputs-1). This returns the cross-entropy\n",
    "    #scalar value for each instance.\n",
    "    #Use 'softmax_cross_entropy_with_logits()' if the labels are in the form of one-hot vectors.\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\") #computes the mean over the mini_batch.\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    #Defining the trainable variables:\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"(dnn/hidden[23]) || (dnn/outputs)\")\n",
    "    #Note that hidden1 weights are not allowed to change. Only weights between h1 and h2 ('hidden2'), between h2 and f3\n",
    "    #('hidden3') ans between h3 and out ('outputs').\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1) #returns boolean == True if the output y is in the first k highest probabilities.  \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) #computes the accuracy.\n",
    "\n",
    "# 3- Node that initialize the variables.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 4- Creating the saver.\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"dnn/hidden[12]\") #scope accept a regular expression.\n",
    "#In this case, we are loading the weights under 'hidden1' and 'hidden2'. Only 'hidden1' will be frozen, 'hidden2' will be used\n",
    "#as initial values only.\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "#print('Original vars:', reuse_vars_dict)\n",
    "original_saver = tf.train.Saver(reuse_vars_dict) # saver to restore the original model\n",
    "new_saver = tf.train.Saver() # saver to save the new model\n",
    "\n",
    "# TF EXECUTION PHASE:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    #intead of sess.run(init)..\n",
    "    sess.run(init) #initializes all variables\n",
    "    original_saver.restore(sess, best_model_path) #restores some variables.\n",
    "    \n",
    "    for epoch in range(n_epochs+1): # for each epoch..\n",
    "        \n",
    "        idXs = np.random.permutation(range(m))\n",
    "        \n",
    "        for batch_index in range(n_batches-1): # for each mini-batch..\n",
    "            X_batch, y_batch = tf_batch(epoch, batch_index, batch_size, idXs, X_train, y_train)\n",
    "            #Training step:\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        if (prt) and (epoch % print_step == 0):\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_train, y: y_train})\n",
    "            print(\"Epoch\", epoch, \"Training accuracy:\", acc_train)\n",
    "        if (save_ckpt) and (epoch % saver_step == 0):\n",
    "            save_path = saver_save.save(sess, \"./tmp/my_model-{}.ckpt\".format(epoch))\n",
    "        \n",
    "        Z = logits.eval(feed_dict={X: X_test})\n",
    "        y_prob = softmax.eval(feed_dict={X: X_test})\n",
    "        y_pred = np.argmax(Z, axis=1) #or np.argmax(y_prob, axis=1)\n",
    "    \n",
    "    #Saving the model\n",
    "    save_path = saver_save.save(sess, \"./tmp/best_2_model-{}.ckpt\".format(now)) \n",
    "\n",
    "print(\"Training time: %.6s seconds\" % (time.time() - start_time))\n",
    "print('Accuracy in the test set: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Results\n",
    "\n",
    "The aim of this document is to present the use of TF to train a net using pre-trained weights. Some prelimineary results were observed. This results are not conclusive and should not be generalized.\n",
    "\n",
    "- Training with all (~1000) examples:\n",
    "    - We still got 100% accuracy, however, more epochs were necessary.\n",
    "    - The training time was decreased from 55s to 46s (a 20% reduction). \n",
    "    - The test accuracies were approximatly the same: 0.99.\n",
    "- Training with 100 examples:\n",
    "    - Training were very fast with and without pre-training.\n",
    "    - Even without pre-training, it was possible to overfit the training data; the accuracy in the test set was 0.92.\n",
    "    - With pre-training, the test set accuracy were 0.9.\n",
    "    - Because the training set is small, it becomes less representative of the true hand-written behavior, hence, overffiting is more severe.\n",
    "- Training with 50 examples:\n",
    "    - Without pre-training the net was not possible to learn to recognize 8's and 9's as different digits very well. The training and test sets accuracies become around 0.6. The accuracy in the training set was low probably because the fixed number of epochs of 100.\n",
    "    - With pre-training it was indeed possible to solve the recognition task even with 100 epochs for training. It was possible to get a earfect score on the training set and 0.84 accuracy on the test set.\n",
    "    \n",
    "It was possible to see from the preliminary runs that the training time is not entirely proportional to the weights in the net. A reduction of 89% in the weights (from 187k to 20k) result in a training time 20% slower. It seems better to avoid pre-training if you have enough time available. However, to create deep structures with very little data, pre-training weights may be necessary.\n",
    "\n",
    "A couple of general questions are still open (the answers for these questions depend much on the problem domain you are working with):\n",
    "\n",
    "- What's the effect on training time, generalization ability and fitness, for different ratios of freezing layers? \n",
    "- What's the difference on training time, generalization ability and fitness when initializing weights as pre-trained values instead of random strategies?\n",
    "- In problems without much data, is it preferable to create a smaller net or a big net with pre-trained weights?\n",
    "- How similar the problems must be to pre-training be useful?\n",
    "- In which problem conditions should we use pre-training weights?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
