{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Tensorflow using Prechelt Early Stopping\n",
    "<p class='lead'>\n",
    "Author: Oliveira, Markos F. B. G.<br />\n",
    "</p>\n",
    "\n",
    "# Description\n",
    "\n",
    "This document is based on Chapter 9 content of Geron's book *Hands-On Machine Learning with Scikit-Learn & TensorFlow*. It covers only the basics of the related topics and it's not meant to be a tutorial. Please, refer to the referenced book and scikit-learn/tensorflow online documentation for specific information.\n",
    "\n",
    "In paticular, this notebook provides an example of applying linear regression using Tensorflow on moderated-size regression problem. It applies stochastic gradient descent using mini-batches of a given size, two approaches for selecting the next mini-batch exist: *1*: random indices from the training set are selected to be inside the mini-batch using a randomized seed, which is the number of the training iteration. In this case, one epoch, in general, does not cover all the training instances. *2*: a random permutation of the examples is passed and a particular slice of this permutation is used as mini-batch. It's guarateed that all examples are used in any epoch (this approach is the most common).\n",
    "\n",
    "The algorithm that minimizes the cost function is a plain stochastic gradient descent with mean squared error. An automatic search for good hyperparameters is not implemented, nor ensemble methods are used. However, it implements three stopping criterias presented in *Early Stopping - But When?* paper written by Lutz Prechelt. Finally, the model is retrained using all set of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching and Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['feature_names', 'DESCR', 'data', 'target'])\n",
      "Entire dataset shape:  (20640, 8)\n"
     ]
    }
   ],
   "source": [
    "#Fetching the data: in the first time it's executed, the data is downloaded to ‘~/scikit_learn_data’ subfolders (this may\n",
    "#take a while); once, downloaded and executed again, the function just read the available dataset.\n",
    "housing = fetch_california_housing()\n",
    "print(fetch_california_housing().keys())\n",
    "print('Entire dataset shape: ', housing.data.shape)\n",
    "\n",
    "#Splitting the dataset into training, validation (to early stopping and testing):\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(housing.data, housing.target, random_state=0, test_size=.2)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=0, test_size=.2) #25% of the 80%\n",
    "#is 20% of the original, so: X_train_ratio = 0.6, X_val_ratio = 0.2, X_test_ratio = 0.2\n",
    "\n",
    "#Normalizing the data:\n",
    "scaler = StandardScaler() #instantiates the scaler.\n",
    "scaler.fit(X_train) #fits the training data, i.e. computes the necessary parameters.\n",
    "X_train_scaled = scaler.transform(X_train) #Transform the datasets based on parameters evaluated on training set only.\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#It's important to transform the y_train and y_test 1d-array ((m,) format) to 2d-array ((m,1) format):\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "#Adding the bias feature to each example (necessary for linear regression):\n",
    "X_train_scaled_plus_bias = np.c_[np.ones((X_train_scaled.shape[0], 1)), X_train_scaled]\n",
    "X_val_scaled_plus_bias = np.c_[np.ones((X_val_scaled.shape[0], 1)), X_val_scaled]\n",
    "X_test_scaled_plus_bias = np.c_[np.ones((X_test_scaled.shape[0], 1)), X_test_scaled]\n",
    "m, n = X_train_scaled_plus_bias.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size, idxs, Xs, ys, approach=2):\n",
    "    \"\"\"Returns the mini-batch (X, y) for a training step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : int\n",
    "        Number of epoch.\n",
    "    batch_index : int\n",
    "        Number of batch inside the epoch.\n",
    "    batch_size : int\n",
    "        Batch size.\n",
    "    idxs : list of int\n",
    "        Permutation of training set indices.\n",
    "    approach: int, optional\n",
    "        Approach used for chosing the mini-batches.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_batch : np.ndarray\n",
    "        Mini-batch of inputs.\n",
    "    y_batch : np.ndarray\n",
    "        Mini-batch of outputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    if approach == 1:\n",
    "        rnd.seed(epoch * n_batches + batch_index)\n",
    "        indices = rnd.randint(m, size=batch_size)\n",
    "    elif approach == 2:\n",
    "        indices = idxs[batch_index * batch_size: (batch_index + 1) * batch_size]\n",
    "    \n",
    "    X_batch = Xs[indices]\n",
    "    y_batch = ys[indices]        \n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early-stopping class\n",
    "\n",
    "Below, the `PrecheltEarlyStopping` class is implemented. It implements stopping criterias presented in *Early Stopping - But When?* paper written by Lutz Prechelt in 1996. Three stopping criteria are presented:\n",
    "\n",
    "<ol>\n",
    "  <li>\n",
    "  Generelization Loss (GL): it measures how much the validation error increased with respect to minimum validation error observed so far (see formula inside the class). This values goes from 0 to +inf. GL = 0 indicates that in the last iteration the validation error was the minimum. GL = 1, indicates that the Eval(t) is 100% (double) above Eopt. See that it doesn't count how many iterations have passed after Eopt; so, if alpha is very big, it can be the case that algorithm never stop (until a upper bound on number of iterations is reached).\n",
    "  </li>\n",
    "  <li>\n",
    "  Generelization Loss / Progress (PQ): progress measures how much was the average error during the last k-length-strip of iterations larger than the minimum training error during the strip. The PQ is defined as the ratio of GL and progress. This criteria is inverselly proportional to progress. Note that in early stage of training, where it's expected that the minimum of the strip is significanty lower than the others (probably previous iterations), the progress is big, which proportionally increases the threshold alpha. In later stages of learning where, probably, all values are common in the strip, the ratio approaches one and the progress approaches zero; this scales down the threshold alpha proportionally. We can see this approach as having a dynamic threshold value for GL according to the oscilations inside the strip (more oscilations means a larger threshold).\n",
    "  The original approch have a '-1' at the end of 'progress' which makes this metric lower than one if the minimum validation error is below half the final one in the strip. If 'progress' is slower than one, then the aalpha threshold is scaled down proportionally, creating a very fast stopping criteria. The '-1' was then removed so that the 'progress' could only scale up the threshold in early and unstable iterations of training, but never scaling down.\n",
    "  </li>\n",
    "  <li>\n",
    "  UP: it stops the procedure when the generalization error increased in s successive strips (not epochs). It only measures the end of the strips, thus, it tries to capure a general trend error increasing.\n",
    "  </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PrecheltEarlyStopping():\n",
    "    \"\"\"Monitor for early stopping in Gradient Boosting for classification.\n",
    "\n",
    "    The monitor checks the validation loss between each training stage. When\n",
    "    too many successive stages have increased the loss, the monitor will return\n",
    "    true, stopping the training early.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_valid : array-like, shape = [n_samples, n_features]\n",
    "      Training vectors, where n_samples is the number of samples\n",
    "      and n_features is the number of features.\n",
    "    y_valid : array-like, shape = [n_samples]\n",
    "      Target values (integers in classification, real numbers in\n",
    "      regression)\n",
    "      For classification, labels must correspond to classes.\n",
    "    max_consecutive_decreases : int, optional (default=5)\n",
    "      Early stopping criteria: when the number of consecutive iterations that\n",
    "      result in a worse performance on the validation set exceeds this value,\n",
    "      the training stops.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Save : Boolean\n",
    "        Save current model.\n",
    "    Stop : Boolean\n",
    "        Stop training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, approach = 1, gl_alpha = 1, pq_alpha = 2, pq_strip = 10, up_strip = 10, up_s = 3):\n",
    "        self.approach = approach\n",
    "        self.gl_alpha = gl_alpha\n",
    "        self.pq_alpha = pq_alpha\n",
    "        self.pq_strip = pq_strip\n",
    "        self.up_strip = up_strip\n",
    "        self.up_s = up_s\n",
    "        self.Eopt = float('inf')\n",
    "        self.strip = []\n",
    "        \n",
    "    def early_stopping(self, Eval):\n",
    "        if Eval <= self.Eopt:\n",
    "            self.Eopt = Eval\n",
    "            return True, False #if a new Eopt is found, no criteria will stop training.\n",
    "        \n",
    "        if self.approach == 1: #Generalization Loss\n",
    "            GL = (Eval/self.Eopt) - 1\n",
    "            if GL > gl_alpha:\n",
    "                return False, True\n",
    "            else:\n",
    "                return False, False        \n",
    "        elif self.approach == 2: #Generalization Loss / Progress (modified to be slow)\n",
    "            #Each entry of the strip is an Eval value evaluated when 'early_stopping' is called. If this method is called\n",
    "            #every N epochs, the strip will have Eval entries spaced by N-1 epochs.\n",
    "            if len(self.strip) < pq_strip: #In early stages of training where 'early_stopping' function were called N times,\n",
    "                #where N is less than 'up_strip' (length of the strip), Eval is just appended in strip attribute.\n",
    "                self.strip.append(Eval)\n",
    "                return False, False\n",
    "            else:\n",
    "                self.strip.pop(0), self.strip.append(Eval)\n",
    "                Prog = (sum(self.strip)/(len(self.strip)*min(self.strip))) # put -1 in the end of formula to get a very fast\n",
    "                #stopping criteria\n",
    "                GL = (Eval/self.Eopt) - 1\n",
    "                PQ = GL/Prog\n",
    "                if PQ > pq_alpha:\n",
    "                    return False, True\n",
    "                else:\n",
    "                    return False, False\n",
    "        elif self.approach == 3: #UP\n",
    "            if len(self.strip) < up_strip*self.up_s:\n",
    "            #Before up_strip*s calls, this criteria could note decide if stop or not, it's necessary a minimum of\n",
    "            #s*strips. \n",
    "                self.strip.append(Eval)\n",
    "                return False, False\n",
    "            else:\n",
    "                self.strip.pop(0), self.strip.append(Eval)\n",
    "                end_strips = [self.strip[i] for i in range(len(self.strip)) if i%self.up_s == 0]\n",
    "                if sorted(end_strips) == end_strips: #the error only increases\n",
    "                    return False, True\n",
    "                else:\n",
    "                    return False, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression using TensorFlow\n",
    "\n",
    "Below, the user can set some running parameters acording to their preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SGD parameters:\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.005\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "#Prechelt Early-stopping:\n",
    "early_stopping_step = 1 #spaced steps in number of epochs that early stopping is checked (it affects PQ and UP approaches).\n",
    "early_stopping = True #enable early stopping.\n",
    "stop_learning = True #enable stop learning.\n",
    "approach = 2 #approach used in early stopping.\n",
    "gl_alpha = 0.1; pq_alpha = 0.06; pq_strip = 10; up_strip = 5; up_s = 2 #parameters of different sopping criterias.\n",
    "min_val_error = float(\"inf\") #stores the best error so far.\n",
    "min_val_error_epoch = 0 #epoch when minimum validation error was found.\n",
    "\n",
    "#Monitoring:\n",
    "prt = True #enable print training statistics (in this case MSE).\n",
    "print_step = 100 # number of epochs to periodically print training statistics.\n",
    "\n",
    "#Saving/restoring TF model:\n",
    "restore = False #enable model's restoration.\n",
    "path_restore = '/tmp/finals/my_model_final.ckpt' #path of original model to be restored.\n",
    "save_ckpt = False #enable saving training checkpoints.\n",
    "saver_step = 100 # number of epochs to periodically save model's checkpoint.\n",
    "\n",
    "#Tensorboard logs:\n",
    "tb = False #enable log training statistics for tensorboard.\n",
    "tensorboard_step = 100 #number of epochs to periodically log statistics in tensorboard log files.\n",
    "root_logdir = \"tf_logs\" #external folder where all logging stats will be placed (for different session runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training MSE = 1.378, Validation MSE = 1.524\n",
      "Epoch 100, Training MSE = 0.5226, Validation MSE = 0.5296\n",
      "Epoch 200, Training MSE = 0.5276, Validation MSE = 0.5475\n",
      "Epoch 300, Training MSE = 0.5272, Validation MSE = 0.5465\n",
      "Epoch 400, Training MSE = 0.5225, Validation MSE = 0.5309\n",
      "Epoch 160 with minimum validation error 0.5246.\n",
      "Best model saved as: ./tmp/best_model-20170815205401.ckpt\n",
      "Training time: 25.57401 seconds\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #restoring the default graph.\n",
    "tf.logging.set_verbosity(tf.logging.WARN) #supress TF logging messages when saving ckpt files.\n",
    "start_time = time.time()\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now) #relative path of tensorboard logs for a particular run (current time\n",
    "#is used so that each folder has different running stats, comparision between them can be made inside tensorboard).\n",
    "\n",
    "# TF CONSTRUCTION PHASE\n",
    "\n",
    "# 1- Creating variables, placeholders and constants.\n",
    "X = tf.placeholder(tf.float32, shape=(None, n), name=\"X\") #n contains the bias already.\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0, seed=1), name=\"theta\")\n",
    "\n",
    "# 2- Creating operations.\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "with tf.name_scope(\"loss\") as scope: # Grouping related nodes to the same scope in the TF-graph.\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# 3- Node that initialize the variables.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 4- Creating the saver.\n",
    "saver_save = tf.train.Saver(max_to_keep=None) #save all variable values.\n",
    "if restore:\n",
    "    saver_restore = tf.train.Saver() #restore all variable values.\n",
    "    #saver_restore = tf.train.Saver({\"weights\": theta}) #restore only the old theta variable under the name of 'weights'.\n",
    "\n",
    "# 5- Tensorboard definitions:\n",
    "if tb:\n",
    "    mse_summary = tf.summary.scalar('MSE', mse) #creates a node in the graph that will evaluate the MSE value\n",
    "    #and write it to a TensorBoard-compatible binary log string called a summary.\n",
    "    summary_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) #creates a FileWriter that you will\n",
    "    #use to write summaries to logfiles in the log directory.\n",
    "\n",
    "\n",
    "# TF EXECUTION PHASE:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if restore:  #to restore a model the construction phase must be identical than the one used to save it. \n",
    "        saver_restore.restore(sess, path_restore)\n",
    "    else:\n",
    "        sess.run(init) #initializing the variables.\n",
    "    \n",
    "    es = PrecheltEarlyStopping(approach = approach, gl_alpha = gl_alpha, pq_alpha = pq_alpha,\n",
    "                              pq_strip = pq_strip, up_strip = up_strip, up_s = up_s) #instantiating early-stopping object.\n",
    "    breaker = False #used to break the epoch loop if the number of iterations without validation improvement increases above\n",
    "    #the threshold 'n_int'.\n",
    "    for epoch in range(n_epochs+1): #for each epoch..\n",
    "        \n",
    "        idXs = np.random.permutation(range(m)) #creates a random permutation of the instances.\n",
    "        \n",
    "        for batch_index in range(n_batches-1): # for each mini-batch.. The '-1' guarantees all batches are equally sized, including\n",
    "            #the last one, without this the last batch with few examples could be used to update the weights. Hence, a few points\n",
    "            #(< batch_size) may be out of each epoch.\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size, idXs, X_train_scaled_plus_bias, y_train)\n",
    "            #Training step:\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "        if (early_stopping) and (epoch % early_stopping_step == 0):\n",
    "            Eval = mse.eval(feed_dict={X: X_val_scaled_plus_bias, y: y_val})\n",
    "            save, stop = es.early_stopping(Eval)\n",
    "            if save:\n",
    "                save_path = saver_save.save(sess, \"./tmp/best_model-{}.ckpt\".format(now))\n",
    "                min_val_error = Eval\n",
    "                min_val_error_epoch = epoch #epoch when minimum validation error was found.\n",
    "            elif stop_learning and stop:\n",
    "                breaker = True\n",
    "                break \n",
    "                    \n",
    "        if breaker:\n",
    "            break \n",
    "        if (prt) and (epoch % print_step == 0):\n",
    "            print(\"Epoch {}, Training MSE = {:.4}, Validation MSE = {:.4}\".format(epoch,\n",
    "                                                                     mse.eval(feed_dict={X: X_train_scaled_plus_bias, y: y_train}),\n",
    "                                                                     mse.eval(feed_dict={X: X_val_scaled_plus_bias, y: y_val})))\n",
    "        if (tb) and (epoch % tensorboard_step == 0):\n",
    "            summary_str_val = mse_summary.eval(feed_dict={X: X_val_scaled_plus_bias, y: y_cal})   \n",
    "            summary_str_training = mse_summary.eval(feed_dict={X: X_train_scaled_plus_bias, y: y_train}) # This will output a\n",
    "            #summary that you can then write to the events file using the file_writer.\n",
    "            step = epoch * n_batches + batch_index\n",
    "            summary_writer.add_summary(summary_str_training, step)\n",
    "            summary_writer.add_summary(summary_str_val, step)\n",
    "        if (save_ckpt) and (epoch % saver_step == 0):\n",
    "            save_path = saver_save.save(sess, \"./tmp/my_model-{}.ckpt\".format(epoch))\n",
    "          \n",
    "    #Getting the 'best theta': this will probably not be the best theta over the training set (forget about the test\n",
    "    #set), because we are updating the weights according the mini batches instead of the full training set error.\n",
    "    #best_theta = theta.eval()\n",
    "    #final_error = mse.eval(feed_dict={X: X_test_scaled_plus_bias, y: y_test})\n",
    "    if not (early_stopping):\n",
    "        save_path = saver_save.save(sess, \"./tmp/best_model-{}.ckpt\".format(now))\n",
    "\n",
    "#Flushing and closing FileWriter if necessary:          \n",
    "if tb:                            \n",
    "    summary_writer.flush()\n",
    "    summary_writer.close() \n",
    "\n",
    "if early_stopping:\n",
    "    print('Epoch {} with minimum validation error {:.4}.'.format(min_val_error_epoch, min_val_error))\n",
    "best_model_path = \"./tmp/best_model-{}.ckpt\".format(now)\n",
    "print(\"Best model saved as:\", best_model_path)\n",
    "print(\"Training time: %.8s seconds\" % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*-*-*-*-*-*-* Final test results *-*-*-*-*-*-*\n",
      "Best theta:\n",
      " [[ 2.06496239]\n",
      " [ 0.83703643]\n",
      " [ 0.11012188]\n",
      " [-0.23900716]\n",
      " [ 0.28375247]\n",
      " [-0.00859088]\n",
      " [ 0.00415122]\n",
      " [-0.8619861 ]\n",
      " [-0.83345944]]\n",
      "Final test set error:  0.534368\n"
     ]
    }
   ],
   "source": [
    "saver_restore = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver_restore.restore(sess, best_model_path) #restoring the previous model saved.\n",
    "    best_theta = theta.eval()\n",
    "    final_error = mse.eval(feed_dict={X: X_test_scaled_plus_bias, y: y_test})\n",
    "\n",
    "print(\"\\n*-*-*-*-*-*-* Final test results *-*-*-*-*-*-*\") \n",
    "print(\"Best theta:\\n\", best_theta)\n",
    "print(\"Final test set error: \", final_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the final model (in the entire dataset available)\n",
    "\n",
    "For better practical results, it's better to implement the model in the entire dataset available. How much the knowledge of the epoch which minimum validation error could be generalized when training the same model, with the entire data set available? i.e., considering the validation and test set too.\n",
    "It's common to separate the entire data set (N) available into training (TR), validation (VL) and testing (TS) sets, with proportions 0.6, 0.2 and 0.2 respectively (these values can slighly change).\n",
    "Considering a training procedure occuring in 60% of the total data, where the minimum validation error occured in epoch Ep. Considering a model with lots of degrees of freedom like neural networks, the early stoppping procedure, avoided overfitting or fitting the particularities of the points available during training that does not encodes intrinsic patterns of the whole population. Increasing the data by a significantly amount, the number of epochs to the overfitting begins will increase, in general. Thus, we could slighly increase the number of epochs during training to capture this phenomena. However,  it's not clear how we should increase the dataset according to individual sets cardinalities. Experiments have to be done to find such factor; in addition it's necessary to compare an approach that only returns the model trained using TR samples, and the one retrained on all dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalizing all the dataset.\n",
    "housing = fetch_california_housing()\n",
    "scaler = StandardScaler() #instantiates the scaler.\n",
    "scaler.fit(housing.data)\n",
    "X_total = scaler.transform(housing.data)\n",
    "X_total_plus_bias = np.c_[np.ones((X_total.shape[0], 1)), X_total]\n",
    "y_total = housing.target.reshape(-1, 1)\n",
    "m, n = X_total_plus_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New number of epochs:  160\n",
      "Final model successifully trained and saved.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() #restoring the default graph.\n",
    "tf.logging.set_verbosity(tf.logging.WARN) #supress TF logging messages when saving ckpt files.\n",
    "start_time = time.time()\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "# TF CONSTRUCTION PHASE\n",
    "\n",
    "# 1- Creating variables, placeholders and constants.\n",
    "X = tf.placeholder(tf.float32, shape=(None, n), name=\"X\") #n contains the bias already.\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n, 1], -1.0, 1.0, seed=1), name=\"theta\")\n",
    "\n",
    "# 2- Creating operations.\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "with tf.name_scope(\"loss\") as scope: # Grouping related nodes to the same scope in the TF-graph.\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "# 3- Node that initialize the variables.\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# 4- Creating the saver.\n",
    "saver_save = tf.train.Saver(max_to_keep=None) #save all variable values.\n",
    "\n",
    "factor = 1\n",
    "n_epochs = int(min_val_error_epoch*factor)\n",
    "print('New number of epochs: ', n_epochs)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init) #initializing the variables.\n",
    "    \n",
    "    for epoch in range(n_epochs): #for each epoch..\n",
    "        \n",
    "        idXs = np.random.permutation(range(m)) #creates a random permutation of the instances.\n",
    "        \n",
    "        for batch_index in range(n_batches-1):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size, idXs, Xs=X_total_plus_bias, ys=y_total)\n",
    "            #Training step:\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "    save_path = saver_save.save(sess, \"./tmp/final_model-{}.ckpt\".format(now))\n",
    "    print('Final model successifully trained and saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
